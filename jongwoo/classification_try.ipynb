{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "okVD4QFXf9fI",
        "fMYEWl13gfrt",
        "cz6hEc_XgaBt",
        "YUq_XuRopHJP",
        "0KU0CB_guqbw",
        "2aOiWdrCruUI",
        "P5HhwcnwCtTS",
        "xgucQ8DxE3yk"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 임종우 최종 정리\n",
        "## 데이터 처리\n",
        "- 1. ViTPose를 통해 keypoint 뽑아내기\n",
        "- 2. 좌우 반전 데이터 증강 적용\n",
        "- 3. 각 sample당 x좌표, y좌표의 평균 및 표준편차 구해 standardization 적용\n",
        "\n",
        "## 모델링\n",
        "\n",
        "### Without CNN\n",
        "- 1. RandomForest -> acc 0.969\n",
        "- 2. Multi Layer Perceptron -> acc 0.94\n",
        "\n",
        "### With CNN\n",
        " : CNN의 경우 (25,2) 형태의 keypoint 데이터를 활용하였음\n",
        "- 3. CNN with Conv1d -> acc 0.94\n",
        "- 4. CNN with Conv2d -> acc 0.95\n",
        "- 5. Pre-trained CNN(MobileNet_v2) -> acc 0.91"
      ],
      "metadata": {
        "id": "pOWjxD5gEoqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 임포트 및 드라이브 마운트"
      ],
      "metadata": {
        "id": "okVD4QFXf9fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "uld8UoCqlB14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0d03c6-abe1-4164-d197-8cacf008df63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssZOhe57kxgL"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From TaekwonDataset infer keypoint"
      ],
      "metadata": {
        "id": "fMYEWl13gfrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keypoints = np.load(\"/content/gdrive/MyDrive/kubig_pose/keypoints.npy\", allow_pickle=True)\n",
        "labels = np.load(\"/content/gdrive/MyDrive/kubig_pose/labels.npy\", allow_pickle=True)"
      ],
      "metadata": {
        "id": "TAQbinm4Vus5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From keypoint to classification"
      ],
      "metadata": {
        "id": "cz6hEc_XgaBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def keypoint_flatten(keypoints, label):\n",
        "  columns = []\n",
        "\n",
        "  for i in range(25):\n",
        "    columns.extend([f'y{i}', f'x{i}'])\n",
        "  columns.append('label')\n",
        "  ml_data = pd.DataFrame(columns = columns)\n",
        "\n",
        "\n",
        "  for idx, keypoint in enumerate(keypoints):\n",
        "    flattend_data = []\n",
        "\n",
        "    try :\n",
        "      for i in keypoint[0]:\n",
        "        flattend_data.append(i[0])\n",
        "        flattend_data.append(i[1])\n",
        "      flattend_data.append(label[idx])\n",
        "      ml_data.loc[idx] = flattend_data\n",
        "    except :\n",
        "      pass\n",
        "  return ml_data"
      ],
      "metadata": {
        "id": "-ZkHRl5DqgUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = keypoint_flatten(keypoints, labels)\n",
        "x = df.drop(columns = 'label')\n",
        "y = df['label']"
      ],
      "metadata": {
        "id": "Gu3LKbiUrlXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 증강 by Flipping Keypoints"
      ],
      "metadata": {
        "id": "k3S9dKc8bnP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 새로운 DataFrame 생성하여 추가할 데이터 저장\n",
        "augmented_data = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    augmented_row = {}\n",
        "    for col_name, value in row.items():\n",
        "        if col_name.startswith('x'):\n",
        "            new_value = 1920 - value  # 1920에서 뺀 값으로 대체\n",
        "            augmented_row[col_name] = new_value\n",
        "        else:\n",
        "            augmented_row[col_name] = value\n",
        "    augmented_data.append(augmented_row)\n",
        "\n",
        "# 새로운 데이터를 포함한 DataFrame 생성\n",
        "augmented_df = pd.DataFrame(augmented_data)\n",
        "\n",
        "# 원래 데이터와 확장된 데이터 결합\n",
        "final_df = pd.concat([df, augmented_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "OjbRVy6vZhfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = final_df.drop(columns = 'label')\n",
        "y = final_df['label']"
      ],
      "metadata": {
        "id": "bTodnTKMaDQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. RandomForest\n",
        "- 0.92\n",
        "- 0.969 with standardization of coordinates"
      ],
      "metadata": {
        "id": "YUq_XuRopHJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x, y, stratify = y, test_size = 0.15, random_state = 42)"
      ],
      "metadata": {
        "id": "5brstC-eu83n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(random_state = 42)"
      ],
      "metadata": {
        "id": "H4KUcl0SkMI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "UG6pF0LgliDV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "499aaedc-bde9-4649-91e8-a0b9a95d34f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(valid_x, valid_y)"
      ],
      "metadata": {
        "id": "CNfq6TIRvNee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9afda814-561b-4111-c0dd-5afac0f8d992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9206521739130434"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Standardization of coordinates\n",
        "- 이미지 내에서 bbox의 위치도 상관없어지고, 인물의 체형도 상관없어지는 효과 있을 것으로 기대\n",
        "- 성능 0.92 -> 0.96으로 상승"
      ],
      "metadata": {
        "id": "JKzBsaF1hfsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standardized_df = final_df.copy()\n",
        "\n",
        "def standardization(value, mean, std):\n",
        "  return (value - mean) / std\n",
        "\n",
        "for idx, row in standardized_df.iterrows():\n",
        "    x_coord = []\n",
        "    y_coord = []\n",
        "    for col_name, value in row.items():\n",
        "        if col_name.startswith('x'):\n",
        "          x_coord.append(value)\n",
        "        elif col_name.startswith('y'):\n",
        "          y_coord.append(value)\n",
        "\n",
        "    x_mean = np.array(x_coord).mean()\n",
        "    x_std = np.array(x_coord).std()\n",
        "    y_mean = np.array(y_coord).mean()\n",
        "    y_std = np.array(y_coord).std()\n",
        "\n",
        "    for col_name, value in row.items():\n",
        "        if col_name.startswith('x'):\n",
        "          standardized_df.loc[idx, col_name] = standardization(value, x_mean, x_std)\n",
        "        elif col_name.startswith('y'):\n",
        "          standardized_df.loc[idx, col_name] = standardization(value, y_mean, y_std)\n",
        "\n",
        "standardized_df[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "D3fiVFhghlMo",
        "outputId": "823e15b0-9c3e-4d2b-e6b3-5ebfe746d028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         y0        x0        y1        x1        y2        x2        y3  \\\n",
              "0 -1.211710 -0.084188 -1.273957  0.292913 -1.278465 -0.404937 -1.250055   \n",
              "1 -1.229617 -0.876085 -1.280594 -0.789579 -1.283216 -0.837689 -1.249739   \n",
              "2 -1.223080 -0.091482 -1.283774  0.267598 -1.282121 -0.461548 -1.270756   \n",
              "3 -1.244221 -0.738662 -1.294174 -0.586785 -1.299885 -0.698386 -1.261048   \n",
              "4 -1.229815 -0.046309 -1.288162  0.285960 -1.285892 -0.375895 -1.269560   \n",
              "\n",
              "         x3        y4        x4  ...       x20       y21       x21       y22  \\\n",
              "0  0.818830 -1.259836 -0.874990  ...  0.819882  1.361415  0.474975  1.148089   \n",
              "1 -0.399690 -1.266595 -0.083295  ... -0.739517  1.340933  0.114045  1.225158   \n",
              "2  0.805753 -1.270191 -1.026884  ...  0.874417  1.348272  0.543388  1.169636   \n",
              "3 -0.019007 -1.263030 -0.087514  ... -1.150055  1.346590 -0.074402  1.202012   \n",
              "4  0.764366 -1.263482 -0.878845  ...  0.800977  1.336187  0.507166  1.168678   \n",
              "\n",
              "        x22       y23       x23       y24       x24  label  \n",
              "0 -0.759188  1.111227 -1.109168  1.014218 -0.625431    0.0  \n",
              "1  1.167877  1.196825  1.233221  1.183299  2.159367    0.0  \n",
              "2 -0.758669  1.139483 -1.162166  1.039530 -0.803478    0.0  \n",
              "3  0.816008  1.173670  0.879507  1.189782  1.935486    0.0  \n",
              "4 -0.920245  1.138770 -1.293106  1.040810 -0.741703    0.0  \n",
              "\n",
              "[5 rows x 51 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dee00ad8-3d39-431d-9a22-e8dc5e6b09bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y0</th>\n",
              "      <th>x0</th>\n",
              "      <th>y1</th>\n",
              "      <th>x1</th>\n",
              "      <th>y2</th>\n",
              "      <th>x2</th>\n",
              "      <th>y3</th>\n",
              "      <th>x3</th>\n",
              "      <th>y4</th>\n",
              "      <th>x4</th>\n",
              "      <th>...</th>\n",
              "      <th>x20</th>\n",
              "      <th>y21</th>\n",
              "      <th>x21</th>\n",
              "      <th>y22</th>\n",
              "      <th>x22</th>\n",
              "      <th>y23</th>\n",
              "      <th>x23</th>\n",
              "      <th>y24</th>\n",
              "      <th>x24</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.211710</td>\n",
              "      <td>-0.084188</td>\n",
              "      <td>-1.273957</td>\n",
              "      <td>0.292913</td>\n",
              "      <td>-1.278465</td>\n",
              "      <td>-0.404937</td>\n",
              "      <td>-1.250055</td>\n",
              "      <td>0.818830</td>\n",
              "      <td>-1.259836</td>\n",
              "      <td>-0.874990</td>\n",
              "      <td>...</td>\n",
              "      <td>0.819882</td>\n",
              "      <td>1.361415</td>\n",
              "      <td>0.474975</td>\n",
              "      <td>1.148089</td>\n",
              "      <td>-0.759188</td>\n",
              "      <td>1.111227</td>\n",
              "      <td>-1.109168</td>\n",
              "      <td>1.014218</td>\n",
              "      <td>-0.625431</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.229617</td>\n",
              "      <td>-0.876085</td>\n",
              "      <td>-1.280594</td>\n",
              "      <td>-0.789579</td>\n",
              "      <td>-1.283216</td>\n",
              "      <td>-0.837689</td>\n",
              "      <td>-1.249739</td>\n",
              "      <td>-0.399690</td>\n",
              "      <td>-1.266595</td>\n",
              "      <td>-0.083295</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.739517</td>\n",
              "      <td>1.340933</td>\n",
              "      <td>0.114045</td>\n",
              "      <td>1.225158</td>\n",
              "      <td>1.167877</td>\n",
              "      <td>1.196825</td>\n",
              "      <td>1.233221</td>\n",
              "      <td>1.183299</td>\n",
              "      <td>2.159367</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.223080</td>\n",
              "      <td>-0.091482</td>\n",
              "      <td>-1.283774</td>\n",
              "      <td>0.267598</td>\n",
              "      <td>-1.282121</td>\n",
              "      <td>-0.461548</td>\n",
              "      <td>-1.270756</td>\n",
              "      <td>0.805753</td>\n",
              "      <td>-1.270191</td>\n",
              "      <td>-1.026884</td>\n",
              "      <td>...</td>\n",
              "      <td>0.874417</td>\n",
              "      <td>1.348272</td>\n",
              "      <td>0.543388</td>\n",
              "      <td>1.169636</td>\n",
              "      <td>-0.758669</td>\n",
              "      <td>1.139483</td>\n",
              "      <td>-1.162166</td>\n",
              "      <td>1.039530</td>\n",
              "      <td>-0.803478</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.244221</td>\n",
              "      <td>-0.738662</td>\n",
              "      <td>-1.294174</td>\n",
              "      <td>-0.586785</td>\n",
              "      <td>-1.299885</td>\n",
              "      <td>-0.698386</td>\n",
              "      <td>-1.261048</td>\n",
              "      <td>-0.019007</td>\n",
              "      <td>-1.263030</td>\n",
              "      <td>-0.087514</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.150055</td>\n",
              "      <td>1.346590</td>\n",
              "      <td>-0.074402</td>\n",
              "      <td>1.202012</td>\n",
              "      <td>0.816008</td>\n",
              "      <td>1.173670</td>\n",
              "      <td>0.879507</td>\n",
              "      <td>1.189782</td>\n",
              "      <td>1.935486</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.229815</td>\n",
              "      <td>-0.046309</td>\n",
              "      <td>-1.288162</td>\n",
              "      <td>0.285960</td>\n",
              "      <td>-1.285892</td>\n",
              "      <td>-0.375895</td>\n",
              "      <td>-1.269560</td>\n",
              "      <td>0.764366</td>\n",
              "      <td>-1.263482</td>\n",
              "      <td>-0.878845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.800977</td>\n",
              "      <td>1.336187</td>\n",
              "      <td>0.507166</td>\n",
              "      <td>1.168678</td>\n",
              "      <td>-0.920245</td>\n",
              "      <td>1.138770</td>\n",
              "      <td>-1.293106</td>\n",
              "      <td>1.040810</td>\n",
              "      <td>-0.741703</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 51 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dee00ad8-3d39-431d-9a22-e8dc5e6b09bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dee00ad8-3d39-431d-9a22-e8dc5e6b09bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dee00ad8-3d39-431d-9a22-e8dc5e6b09bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c3e627f0-7dc2-430f-abd4-385d3d8f5604\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c3e627f0-7dc2-430f-abd4-385d3d8f5604')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c3e627f0-7dc2-430f-abd4-385d3d8f5604 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = standardized_df.drop(columns = 'label')\n",
        "y = standardized_df['label']"
      ],
      "metadata": {
        "id": "akXli-Hujbjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x, y, stratify = y, test_size = 0.15, random_state = 42)"
      ],
      "metadata": {
        "id": "MEpOJ9lLjem7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(random_state = 42)"
      ],
      "metadata": {
        "id": "xveryWaUjem7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_x, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "dd7dda51-3e0f-4a46-d75e-e2689bb3b9ec",
        "id": "Rvhv1bOBjem8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(valid_x, valid_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b898ad-ac2a-4834-bd20-ae16a19686b6",
        "id": "FLCHHbF0jem8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9695652173913043"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Basic MLP\n",
        "- Adam, 0.001, hidden 레이어 1개, epoch 100, 0.93\n",
        "- Adam, 0.001, hidden 레이어 2개, epoch 100, 0.93\n",
        "- Adam, 0.001, hidden 레이어 2개, epoch 150, 0.94"
      ],
      "metadata": {
        "id": "0KU0CB_guqbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def from_df_to_tensor(df):\n",
        "  data = []\n",
        "  label = []\n",
        "  for idx, row in df.iterrows():\n",
        "    sample = []\n",
        "    for col_name, value in row.items():\n",
        "      if col_name == 'label' :\n",
        "        label.append(value)\n",
        "      else:\n",
        "        sample.append(value)\n",
        "    data.append(np.array(sample))\n",
        "  return np.array(data), np.array(label)"
      ],
      "metadata": {
        "id": "ISMDvAxXk_NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = from_df_to_tensor(standardized_df)"
      ],
      "metadata": {
        "id": "2bDRS6-m4NF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x, y, stratify = y, test_size = 0.15, random_state = 42)"
      ],
      "metadata": {
        "id": "oU9S68v9_c4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomKeypointsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, keypoints, labels):\n",
        "        self.keypoints = keypoints\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        keypoint = torch.tensor(self.keypoints[idx])\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "        return keypoint, label"
      ],
      "metadata": {
        "id": "MTwPO4IATanA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SkeletonTrainDataset = CustomKeypointsDataset(train_x, train_y)\n",
        "SkeletonValidDataset = CustomKeypointsDataset(valid_x, valid_y)"
      ],
      "metadata": {
        "id": "1-ad7R29ULe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainDataloader = torch.utils.data.DataLoader(SkeletonTrainDataset, batch_size = 128, shuffle = True)\n",
        "ValidDataloader = torch.utils.data.DataLoader(SkeletonValidDataset, batch_size = 128,shuffle = False)"
      ],
      "metadata": {
        "id": "y4mA5kKBUa0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class basic_mlp(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(50, 64, dtype = float)\n",
        "    self.fc2 = nn.Linear(64,128, dtype = float)\n",
        "    self.fc3 = nn.Linear(128, 1024, dtype = float)\n",
        "    self.fc4 = nn.Linear(1024, 47, dtype = float)\n",
        "    self.dropout = nn.Dropout(p = 0.3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc4(x)\n",
        "    # x = F.softmax(x) -> nn.CrossEntropyLoss 내에 softmax 연산이 포함됨\n",
        "    return x"
      ],
      "metadata": {
        "id": "_nGWKbJhpBYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = basic_mlp()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "IA8SpzQyR2Xs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1f1953-4153-4205-f151-61ca3324839e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "basic_mlp(\n",
              "  (fc1): Linear(in_features=50, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=1024, bias=True)\n",
              "  (fc4): Linear(in_features=1024, out_features=47, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 358
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Ylk69dD5Rv8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(150):\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  valid_loss = []\n",
        "  valid_acc = []\n",
        "\n",
        "  for skeleton, cls in iter(TrainDataloader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "\n",
        "    output = model(skeleton)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss.append(loss.item())\n",
        "    train_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for skeleton, cls in iter(ValidDataloader):\n",
        "      skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "      output = model(skeleton)\n",
        "      loss = criterion(output, cls)\n",
        "      valid_loss.append(loss.item())\n",
        "      valid_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "\n",
        "  print(f'epoch {epoch+1} -- train_loss : {sum(train_loss[-len(TrainDataloader):]) / len(TrainDataloader):.5f} \\\n",
        "  train_acc : {sum(train_acc[-len(TrainDataloader):]) / len(TrainDataloader):.5f}\\\n",
        "   valid_loss : {sum(valid_loss[-len(ValidDataloader):]) / len(ValidDataloader):.5f} \\\n",
        "  valid_acc : {sum(valid_acc[-len(ValidDataloader):]) / len(ValidDataloader):.5f}')"
      ],
      "metadata": {
        "id": "KWOQvKls_eLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e6c9e4-827b-4e6d-f090-7e2303c1c864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 -- train_loss : 3.53028   train_acc : 0.14422   valid_loss : 3.36329   valid_acc : 0.15592\n",
            "epoch 2 -- train_loss : 3.02039   train_acc : 0.20380   valid_loss : 2.70718   valid_acc : 0.24740\n",
            "epoch 3 -- train_loss : 2.37480   train_acc : 0.31471   valid_loss : 2.04779   valid_acc : 0.40853\n",
            "epoch 4 -- train_loss : 1.78559   train_acc : 0.45929   valid_loss : 1.58485   valid_acc : 0.51074\n",
            "epoch 5 -- train_loss : 1.38622   train_acc : 0.56780   valid_loss : 1.27651   valid_acc : 0.61198\n",
            "epoch 6 -- train_loss : 1.10935   train_acc : 0.64216   valid_loss : 1.01755   valid_acc : 0.67871\n",
            "epoch 7 -- train_loss : 0.94741   train_acc : 0.68529   valid_loss : 0.88122   valid_acc : 0.71647\n",
            "epoch 8 -- train_loss : 0.80125   train_acc : 0.73414   valid_loss : 0.79781   valid_acc : 0.73275\n",
            "epoch 9 -- train_loss : 0.74019   train_acc : 0.74490   valid_loss : 0.83655   valid_acc : 0.69401\n",
            "epoch 10 -- train_loss : 0.66211   train_acc : 0.77457   valid_loss : 0.70174   valid_acc : 0.75423\n",
            "epoch 11 -- train_loss : 0.59687   train_acc : 0.79222   valid_loss : 0.59086   valid_acc : 0.81120\n",
            "epoch 12 -- train_loss : 0.52808   train_acc : 0.81877   valid_loss : 0.57754   valid_acc : 0.80892\n",
            "epoch 13 -- train_loss : 0.48122   train_acc : 0.83200   valid_loss : 0.60158   valid_acc : 0.80924\n",
            "epoch 14 -- train_loss : 0.46576   train_acc : 0.83120   valid_loss : 0.51141   valid_acc : 0.83203\n",
            "epoch 15 -- train_loss : 0.43726   train_acc : 0.84547   valid_loss : 0.52979   valid_acc : 0.82161\n",
            "epoch 16 -- train_loss : 0.39080   train_acc : 0.86053   valid_loss : 0.45275   valid_acc : 0.85840\n",
            "epoch 17 -- train_loss : 0.37676   train_acc : 0.86748   valid_loss : 0.43300   valid_acc : 0.86165\n",
            "epoch 18 -- train_loss : 0.36044   train_acc : 0.86839   valid_loss : 0.47909   valid_acc : 0.83105\n",
            "epoch 19 -- train_loss : 0.33860   train_acc : 0.87536   valid_loss : 0.44820   valid_acc : 0.84896\n",
            "epoch 20 -- train_loss : 0.32665   train_acc : 0.88001   valid_loss : 0.43668   valid_acc : 0.85514\n",
            "epoch 21 -- train_loss : 0.30626   train_acc : 0.88900   valid_loss : 0.40216   valid_acc : 0.86165\n",
            "epoch 22 -- train_loss : 0.28914   train_acc : 0.89091   valid_loss : 0.39955   valid_acc : 0.87630\n",
            "epoch 23 -- train_loss : 0.27306   train_acc : 0.89929   valid_loss : 0.36589   valid_acc : 0.87630\n",
            "epoch 24 -- train_loss : 0.26185   train_acc : 0.90645   valid_loss : 0.41463   valid_acc : 0.85352\n",
            "epoch 25 -- train_loss : 0.26266   train_acc : 0.90337   valid_loss : 0.34798   valid_acc : 0.89811\n",
            "epoch 26 -- train_loss : 0.24980   train_acc : 0.90188   valid_loss : 0.37943   valid_acc : 0.87044\n",
            "epoch 27 -- train_loss : 0.23244   train_acc : 0.91439   valid_loss : 0.35612   valid_acc : 0.88574\n",
            "epoch 28 -- train_loss : 0.22116   train_acc : 0.91759   valid_loss : 0.37492   valid_acc : 0.87923\n",
            "epoch 29 -- train_loss : 0.20874   train_acc : 0.92139   valid_loss : 0.32273   valid_acc : 0.90234\n",
            "epoch 30 -- train_loss : 0.20799   train_acc : 0.92086   valid_loss : 0.33269   valid_acc : 0.88477\n",
            "epoch 31 -- train_loss : 0.19773   train_acc : 0.93009   valid_loss : 0.32697   valid_acc : 0.89583\n",
            "epoch 32 -- train_loss : 0.20241   train_acc : 0.92021   valid_loss : 0.33138   valid_acc : 0.89193\n",
            "epoch 33 -- train_loss : 0.20051   train_acc : 0.92094   valid_loss : 0.32169   valid_acc : 0.90007\n",
            "epoch 34 -- train_loss : 0.19743   train_acc : 0.92615   valid_loss : 0.29946   valid_acc : 0.90690\n",
            "epoch 35 -- train_loss : 0.18135   train_acc : 0.93176   valid_loss : 0.32123   valid_acc : 0.89844\n",
            "epoch 36 -- train_loss : 0.17140   train_acc : 0.93318   valid_loss : 0.29549   valid_acc : 0.91862\n",
            "epoch 37 -- train_loss : 0.17754   train_acc : 0.93138   valid_loss : 0.32280   valid_acc : 0.88704\n",
            "epoch 38 -- train_loss : 0.17385   train_acc : 0.93255   valid_loss : 0.29978   valid_acc : 0.91764\n",
            "epoch 39 -- train_loss : 0.16870   train_acc : 0.93438   valid_loss : 0.29351   valid_acc : 0.90560\n",
            "epoch 40 -- train_loss : 0.15616   train_acc : 0.93670   valid_loss : 0.27780   valid_acc : 0.91243\n",
            "epoch 41 -- train_loss : 0.15825   train_acc : 0.94086   valid_loss : 0.32092   valid_acc : 0.90007\n",
            "epoch 42 -- train_loss : 0.15202   train_acc : 0.94361   valid_loss : 0.28476   valid_acc : 0.91569\n",
            "epoch 43 -- train_loss : 0.15756   train_acc : 0.94144   valid_loss : 0.30474   valid_acc : 0.90853\n",
            "epoch 44 -- train_loss : 0.14346   train_acc : 0.94589   valid_loss : 0.28593   valid_acc : 0.90658\n",
            "epoch 45 -- train_loss : 0.14151   train_acc : 0.94947   valid_loss : 0.29322   valid_acc : 0.92057\n",
            "epoch 46 -- train_loss : 0.15205   train_acc : 0.94250   valid_loss : 0.28225   valid_acc : 0.91667\n",
            "epoch 47 -- train_loss : 0.13947   train_acc : 0.94551   valid_loss : 0.26181   valid_acc : 0.92155\n",
            "epoch 48 -- train_loss : 0.12208   train_acc : 0.95652   valid_loss : 0.27588   valid_acc : 0.90820\n",
            "epoch 49 -- train_loss : 0.11653   train_acc : 0.95359   valid_loss : 0.29335   valid_acc : 0.90169\n",
            "epoch 50 -- train_loss : 0.13492   train_acc : 0.94746   valid_loss : 0.25044   valid_acc : 0.92969\n",
            "epoch 51 -- train_loss : 0.11232   train_acc : 0.95740   valid_loss : 0.27477   valid_acc : 0.91048\n",
            "epoch 52 -- train_loss : 0.11841   train_acc : 0.95611   valid_loss : 0.30679   valid_acc : 0.90234\n",
            "epoch 53 -- train_loss : 0.11967   train_acc : 0.95233   valid_loss : 0.26596   valid_acc : 0.93066\n",
            "epoch 54 -- train_loss : 0.10409   train_acc : 0.96250   valid_loss : 0.28234   valid_acc : 0.92025\n",
            "epoch 55 -- train_loss : 0.11465   train_acc : 0.95671   valid_loss : 0.30874   valid_acc : 0.91081\n",
            "epoch 56 -- train_loss : 0.11205   train_acc : 0.95294   valid_loss : 0.27559   valid_acc : 0.92383\n",
            "epoch 57 -- train_loss : 0.10736   train_acc : 0.95557   valid_loss : 0.28708   valid_acc : 0.91960\n",
            "epoch 58 -- train_loss : 0.11442   train_acc : 0.95835   valid_loss : 0.32520   valid_acc : 0.90951\n",
            "epoch 59 -- train_loss : 0.11146   train_acc : 0.95747   valid_loss : 0.27286   valid_acc : 0.91178\n",
            "epoch 60 -- train_loss : 0.10952   train_acc : 0.95591   valid_loss : 0.27963   valid_acc : 0.91960\n",
            "epoch 61 -- train_loss : 0.10384   train_acc : 0.95767   valid_loss : 0.26317   valid_acc : 0.91960\n",
            "epoch 62 -- train_loss : 0.10294   train_acc : 0.95938   valid_loss : 0.27722   valid_acc : 0.92253\n",
            "epoch 63 -- train_loss : 0.10548   train_acc : 0.95755   valid_loss : 0.25550   valid_acc : 0.92969\n",
            "epoch 64 -- train_loss : 0.09920   train_acc : 0.96056   valid_loss : 0.24232   valid_acc : 0.92611\n",
            "epoch 65 -- train_loss : 0.10152   train_acc : 0.95980   valid_loss : 0.25704   valid_acc : 0.92448\n",
            "epoch 66 -- train_loss : 0.12221   train_acc : 0.95298   valid_loss : 0.30887   valid_acc : 0.89844\n",
            "epoch 67 -- train_loss : 0.12403   train_acc : 0.95032   valid_loss : 0.32043   valid_acc : 0.89844\n",
            "epoch 68 -- train_loss : 0.10456   train_acc : 0.95725   valid_loss : 0.23458   valid_acc : 0.92350\n",
            "epoch 69 -- train_loss : 0.09417   train_acc : 0.96308   valid_loss : 0.24571   valid_acc : 0.93327\n",
            "epoch 70 -- train_loss : 0.10317   train_acc : 0.95980   valid_loss : 0.26517   valid_acc : 0.92611\n",
            "epoch 71 -- train_loss : 0.11170   train_acc : 0.95519   valid_loss : 0.30735   valid_acc : 0.91178\n",
            "epoch 72 -- train_loss : 0.10499   train_acc : 0.95965   valid_loss : 0.27321   valid_acc : 0.92839\n",
            "epoch 73 -- train_loss : 0.08396   train_acc : 0.96906   valid_loss : 0.26725   valid_acc : 0.92806\n",
            "epoch 74 -- train_loss : 0.08922   train_acc : 0.96357   valid_loss : 0.29909   valid_acc : 0.92546\n",
            "epoch 75 -- train_loss : 0.09116   train_acc : 0.96372   valid_loss : 0.29956   valid_acc : 0.92318\n",
            "epoch 76 -- train_loss : 0.09127   train_acc : 0.96399   valid_loss : 0.26574   valid_acc : 0.92415\n",
            "epoch 77 -- train_loss : 0.09441   train_acc : 0.96350   valid_loss : 0.25421   valid_acc : 0.92936\n",
            "epoch 78 -- train_loss : 0.07665   train_acc : 0.96974   valid_loss : 0.28599   valid_acc : 0.91992\n",
            "epoch 79 -- train_loss : 0.08395   train_acc : 0.96700   valid_loss : 0.28964   valid_acc : 0.91699\n",
            "epoch 80 -- train_loss : 0.07222   train_acc : 0.97108   valid_loss : 0.25037   valid_acc : 0.92839\n",
            "epoch 81 -- train_loss : 0.08138   train_acc : 0.96834   valid_loss : 0.28423   valid_acc : 0.92806\n",
            "epoch 82 -- train_loss : 0.08068   train_acc : 0.96514   valid_loss : 0.24569   valid_acc : 0.93652\n",
            "epoch 83 -- train_loss : 0.07661   train_acc : 0.96807   valid_loss : 0.28529   valid_acc : 0.93327\n",
            "epoch 84 -- train_loss : 0.07322   train_acc : 0.97097   valid_loss : 0.26932   valid_acc : 0.92643\n",
            "epoch 85 -- train_loss : 0.08715   train_acc : 0.96391   valid_loss : 0.28155   valid_acc : 0.92806\n",
            "epoch 86 -- train_loss : 0.07777   train_acc : 0.96807   valid_loss : 0.24599   valid_acc : 0.93620\n",
            "epoch 87 -- train_loss : 0.07414   train_acc : 0.96926   valid_loss : 0.29465   valid_acc : 0.91504\n",
            "epoch 88 -- train_loss : 0.07757   train_acc : 0.96791   valid_loss : 0.24617   valid_acc : 0.93229\n",
            "epoch 89 -- train_loss : 0.07161   train_acc : 0.97115   valid_loss : 0.25992   valid_acc : 0.94336\n",
            "epoch 90 -- train_loss : 0.07627   train_acc : 0.96829   valid_loss : 0.25965   valid_acc : 0.92643\n",
            "epoch 91 -- train_loss : 0.07941   train_acc : 0.96776   valid_loss : 0.32555   valid_acc : 0.91276\n",
            "epoch 92 -- train_loss : 0.08905   train_acc : 0.96422   valid_loss : 0.26807   valid_acc : 0.93522\n",
            "epoch 93 -- train_loss : 0.08475   train_acc : 0.96567   valid_loss : 0.35388   valid_acc : 0.90169\n",
            "epoch 94 -- train_loss : 0.11116   train_acc : 0.95743   valid_loss : 0.33466   valid_acc : 0.90658\n",
            "epoch 95 -- train_loss : 0.07979   train_acc : 0.96860   valid_loss : 0.28253   valid_acc : 0.92285\n",
            "epoch 96 -- train_loss : 0.07312   train_acc : 0.96914   valid_loss : 0.27325   valid_acc : 0.92936\n",
            "epoch 97 -- train_loss : 0.06994   train_acc : 0.97218   valid_loss : 0.31690   valid_acc : 0.91634\n",
            "epoch 98 -- train_loss : 0.06741   train_acc : 0.97458   valid_loss : 0.27086   valid_acc : 0.94238\n",
            "epoch 99 -- train_loss : 0.06352   train_acc : 0.97573   valid_loss : 0.26584   valid_acc : 0.93522\n",
            "epoch 100 -- train_loss : 0.06309   train_acc : 0.97279   valid_loss : 0.29047   valid_acc : 0.91699\n",
            "epoch 101 -- train_loss : 0.08230   train_acc : 0.96795   valid_loss : 0.27473   valid_acc : 0.93262\n",
            "epoch 102 -- train_loss : 0.06985   train_acc : 0.97405   valid_loss : 0.29619   valid_acc : 0.92318\n",
            "epoch 103 -- train_loss : 0.05792   train_acc : 0.97870   valid_loss : 0.29787   valid_acc : 0.92546\n",
            "epoch 104 -- train_loss : 0.07259   train_acc : 0.96891   valid_loss : 0.24665   valid_acc : 0.93750\n",
            "epoch 105 -- train_loss : 0.07674   train_acc : 0.96841   valid_loss : 0.26813   valid_acc : 0.93522\n",
            "epoch 106 -- train_loss : 0.07677   train_acc : 0.97108   valid_loss : 0.23985   valid_acc : 0.94108\n",
            "epoch 107 -- train_loss : 0.06158   train_acc : 0.97508   valid_loss : 0.26900   valid_acc : 0.93424\n",
            "epoch 108 -- train_loss : 0.05457   train_acc : 0.97775   valid_loss : 0.27330   valid_acc : 0.93913\n",
            "epoch 109 -- train_loss : 0.06156   train_acc : 0.97577   valid_loss : 0.27498   valid_acc : 0.93652\n",
            "epoch 110 -- train_loss : 0.05865   train_acc : 0.97497   valid_loss : 0.24695   valid_acc : 0.94010\n",
            "epoch 111 -- train_loss : 0.05824   train_acc : 0.97748   valid_loss : 0.27704   valid_acc : 0.93945\n",
            "epoch 112 -- train_loss : 0.07282   train_acc : 0.97112   valid_loss : 0.26839   valid_acc : 0.93913\n",
            "epoch 113 -- train_loss : 0.06970   train_acc : 0.97013   valid_loss : 0.30364   valid_acc : 0.93424\n",
            "epoch 114 -- train_loss : 0.07198   train_acc : 0.97371   valid_loss : 0.29501   valid_acc : 0.93164\n",
            "epoch 115 -- train_loss : 0.06405   train_acc : 0.97428   valid_loss : 0.26352   valid_acc : 0.93848\n",
            "epoch 116 -- train_loss : 0.06302   train_acc : 0.97417   valid_loss : 0.27188   valid_acc : 0.94336\n",
            "epoch 117 -- train_loss : 0.05789   train_acc : 0.97752   valid_loss : 0.27556   valid_acc : 0.93945\n",
            "epoch 118 -- train_loss : 0.06806   train_acc : 0.97119   valid_loss : 0.33822   valid_acc : 0.92350\n",
            "epoch 119 -- train_loss : 0.06579   train_acc : 0.97363   valid_loss : 0.27360   valid_acc : 0.93555\n",
            "epoch 120 -- train_loss : 0.05823   train_acc : 0.97786   valid_loss : 0.26963   valid_acc : 0.93262\n",
            "epoch 121 -- train_loss : 0.10249   train_acc : 0.96353   valid_loss : 0.30296   valid_acc : 0.92969\n",
            "epoch 122 -- train_loss : 0.07120   train_acc : 0.97081   valid_loss : 0.25296   valid_acc : 0.94108\n",
            "epoch 123 -- train_loss : 0.07898   train_acc : 0.97264   valid_loss : 0.29404   valid_acc : 0.94727\n",
            "epoch 124 -- train_loss : 0.07643   train_acc : 0.96746   valid_loss : 0.25296   valid_acc : 0.94141\n",
            "epoch 125 -- train_loss : 0.05617   train_acc : 0.97581   valid_loss : 0.27964   valid_acc : 0.94531\n",
            "epoch 126 -- train_loss : 0.04656   train_acc : 0.98209   valid_loss : 0.28126   valid_acc : 0.94531\n",
            "epoch 127 -- train_loss : 0.04201   train_acc : 0.98323   valid_loss : 0.25710   valid_acc : 0.93620\n",
            "epoch 128 -- train_loss : 0.04849   train_acc : 0.98186   valid_loss : 0.28719   valid_acc : 0.93717\n",
            "epoch 129 -- train_loss : 0.05022   train_acc : 0.98053   valid_loss : 0.29314   valid_acc : 0.94629\n",
            "epoch 130 -- train_loss : 0.05442   train_acc : 0.97942   valid_loss : 0.31596   valid_acc : 0.93457\n",
            "epoch 131 -- train_loss : 0.06308   train_acc : 0.97394   valid_loss : 0.28386   valid_acc : 0.94141\n",
            "epoch 132 -- train_loss : 0.05427   train_acc : 0.97935   valid_loss : 0.26967   valid_acc : 0.93750\n",
            "epoch 133 -- train_loss : 0.04843   train_acc : 0.98156   valid_loss : 0.25845   valid_acc : 0.93815\n",
            "epoch 134 -- train_loss : 0.05047   train_acc : 0.98075   valid_loss : 0.26544   valid_acc : 0.92839\n",
            "epoch 135 -- train_loss : 0.05198   train_acc : 0.97916   valid_loss : 0.28272   valid_acc : 0.94368\n",
            "epoch 136 -- train_loss : 0.06379   train_acc : 0.97946   valid_loss : 0.37030   valid_acc : 0.91276\n",
            "epoch 137 -- train_loss : 0.08206   train_acc : 0.96731   valid_loss : 0.31754   valid_acc : 0.93229\n",
            "epoch 138 -- train_loss : 0.06262   train_acc : 0.97398   valid_loss : 0.29643   valid_acc : 0.93229\n",
            "epoch 139 -- train_loss : 0.05242   train_acc : 0.97718   valid_loss : 0.26433   valid_acc : 0.94824\n",
            "epoch 140 -- train_loss : 0.04445   train_acc : 0.98095   valid_loss : 0.27045   valid_acc : 0.93652\n",
            "epoch 141 -- train_loss : 0.05218   train_acc : 0.97981   valid_loss : 0.29198   valid_acc : 0.93913\n",
            "epoch 142 -- train_loss : 0.04625   train_acc : 0.98080   valid_loss : 0.28442   valid_acc : 0.93945\n",
            "epoch 143 -- train_loss : 0.04503   train_acc : 0.98232   valid_loss : 0.26559   valid_acc : 0.93815\n",
            "epoch 144 -- train_loss : 0.04968   train_acc : 0.97927   valid_loss : 0.27823   valid_acc : 0.94010\n",
            "epoch 145 -- train_loss : 0.05944   train_acc : 0.97794   valid_loss : 0.28329   valid_acc : 0.93652\n",
            "epoch 146 -- train_loss : 0.05976   train_acc : 0.97584   valid_loss : 0.29582   valid_acc : 0.92839\n",
            "epoch 147 -- train_loss : 0.04907   train_acc : 0.98175   valid_loss : 0.28906   valid_acc : 0.93913\n",
            "epoch 148 -- train_loss : 0.04153   train_acc : 0.98342   valid_loss : 0.27064   valid_acc : 0.94336\n",
            "epoch 149 -- train_loss : 0.03731   train_acc : 0.98514   valid_loss : 0.30646   valid_acc : 0.93913\n",
            "epoch 150 -- train_loss : 0.03866   train_acc : 0.98423   valid_loss : 0.26531   valid_acc : 0.94629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # CNN(Conv1d)\n",
        " - 여러개의 필터 직렬 적용, epoch 100, 0.94\n",
        " - 여러개의 필터 병렬 적용, epoch 100, 0.92"
      ],
      "metadata": {
        "id": "2aOiWdrCruUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def from_df_to_cnn(df):\n",
        "  data = df.drop(columns = 'label')\n",
        "  labels = df['label']\n",
        "\n",
        "  data_list = []\n",
        "\n",
        "  for idx, rows in data.iterrows():\n",
        "    data_list.append(np.array(rows).reshape(-1,2))\n",
        "\n",
        "  return np.array(data_list), np.array(labels)"
      ],
      "metadata": {
        "id": "9ovBEsZXrvTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = from_df_to_cnn(standardized_df)"
      ],
      "metadata": {
        "id": "0rwVMv-Ir88C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x, y, stratify = y, test_size = 0.15, random_state = 42)"
      ],
      "metadata": {
        "id": "YryERnIctb7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomKeypointsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, keypoints, labels):\n",
        "        self.keypoints = keypoints\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        keypoint = torch.tensor(self.keypoints[idx].T, dtype = torch.float)\n",
        "        label = torch.tensor(self.labels[idx], dtype = torch.float)\n",
        "        return keypoint, label"
      ],
      "metadata": {
        "id": "rzhi7svItWN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SkeletonTrainDataset = CustomKeypointsDataset(train_x, train_y)\n",
        "SkeletonValidDataset = CustomKeypointsDataset(valid_x, valid_y)"
      ],
      "metadata": {
        "id": "ZYdw924wtWOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainDataloader = torch.utils.data.DataLoader(SkeletonTrainDataset, batch_size = 128, shuffle = True)\n",
        "ValidDataloader = torch.utils.data.DataLoader(SkeletonValidDataset, batch_size = 128,shuffle = False)"
      ],
      "metadata": {
        "id": "0vZ_AN-5tWOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic CNN with conv1d\n",
        "\n",
        "class VanilaCNN(nn.Module):\n",
        "  def __init__(self) :\n",
        "    super(VanilaCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(2, 16, 3)\n",
        "    self.conv2 = nn.Conv1d(16, 32, 3)\n",
        "    self.maxpool = nn.MaxPool1d(2)\n",
        "    self.fc1 = nn.Linear(32*4, 128)\n",
        "    self.fc2 = nn.Linear(128,47)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = x.view(-1, 32*4)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "model = VanilaCNN()\n",
        "print(model(next(iter(TrainDataloader))[0]).shape)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbFzFkC1vc98",
        "outputId": "6b29e3db-9dbe-4c12-c15c-cb85c1948d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 47])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VanilaCNN(\n",
              "  (conv1): Conv1d(2, 16, kernel_size=(3,), stride=(1,))\n",
              "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,))\n",
              "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=47, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 452
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  valid_loss = []\n",
        "  valid_acc = []\n",
        "\n",
        "  for skeleton, cls in iter(TrainDataloader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "\n",
        "    output = model(skeleton)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss.append(loss.item())\n",
        "    train_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for skeleton, cls in iter(ValidDataloader):\n",
        "      skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "      output = model(skeleton)\n",
        "      loss = criterion(output, cls)\n",
        "      valid_loss.append(loss.item())\n",
        "      valid_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "\n",
        "  print(f'epoch {epoch+1} -- train_loss : {sum(train_loss[-len(TrainDataloader):]) / len(TrainDataloader):.5f} \\\n",
        "  train_acc : {sum(train_acc[-len(TrainDataloader):]) / len(TrainDataloader):.5f}\\\n",
        "   valid_loss : {sum(valid_loss[-len(ValidDataloader):]) / len(ValidDataloader):.5f} \\\n",
        "  valid_acc : {sum(valid_acc[-len(ValidDataloader):]) / len(ValidDataloader):.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNvhTrNtB7hK",
        "outputId": "28edc336-6145-4cd8-8955-b01e8e107e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 -- train_loss : 3.61090   train_acc : 0.15119   valid_loss : 3.50960   valid_acc : 0.16797\n",
            "epoch 2 -- train_loss : 3.39223   train_acc : 0.17459   valid_loss : 3.29285   valid_acc : 0.17480\n",
            "epoch 3 -- train_loss : 3.06773   train_acc : 0.22793   valid_loss : 2.88734   valid_acc : 0.25358\n",
            "epoch 4 -- train_loss : 2.65635   train_acc : 0.28325   valid_loss : 2.48012   valid_acc : 0.30534\n",
            "epoch 5 -- train_loss : 2.27329   train_acc : 0.35466   valid_loss : 2.14309   valid_acc : 0.38249\n",
            "epoch 6 -- train_loss : 1.95540   train_acc : 0.42421   valid_loss : 1.82259   valid_acc : 0.46354\n",
            "epoch 7 -- train_loss : 1.68365   train_acc : 0.49428   valid_loss : 1.57695   valid_acc : 0.52637\n",
            "epoch 8 -- train_loss : 1.45865   train_acc : 0.55692   valid_loss : 1.39641   valid_acc : 0.59277\n",
            "epoch 9 -- train_loss : 1.27839   train_acc : 0.60393   valid_loss : 1.19914   valid_acc : 0.62630\n",
            "epoch 10 -- train_loss : 1.14055   train_acc : 0.63923   valid_loss : 1.08032   valid_acc : 0.66374\n",
            "epoch 11 -- train_loss : 1.02602   train_acc : 0.67154   valid_loss : 0.97629   valid_acc : 0.70052\n",
            "epoch 12 -- train_loss : 0.91805   train_acc : 0.71247   valid_loss : 0.89104   valid_acc : 0.72624\n",
            "epoch 13 -- train_loss : 0.83858   train_acc : 0.73415   valid_loss : 0.81954   valid_acc : 0.74772\n",
            "epoch 14 -- train_loss : 0.76946   train_acc : 0.75877   valid_loss : 0.76754   valid_acc : 0.76693\n",
            "epoch 15 -- train_loss : 0.70140   train_acc : 0.78150   valid_loss : 0.72199   valid_acc : 0.78320\n",
            "epoch 16 -- train_loss : 0.65334   train_acc : 0.79992   valid_loss : 0.67139   valid_acc : 0.79362\n",
            "epoch 17 -- train_loss : 0.60690   train_acc : 0.81067   valid_loss : 0.62511   valid_acc : 0.80729\n",
            "epoch 18 -- train_loss : 0.56722   train_acc : 0.82030   valid_loss : 0.59040   valid_acc : 0.83431\n",
            "epoch 19 -- train_loss : 0.52641   train_acc : 0.83286   valid_loss : 0.56557   valid_acc : 0.83398\n",
            "epoch 20 -- train_loss : 0.50424   train_acc : 0.83879   valid_loss : 0.52878   valid_acc : 0.84766\n",
            "epoch 21 -- train_loss : 0.46723   train_acc : 0.85464   valid_loss : 0.50184   valid_acc : 0.86491\n",
            "epoch 22 -- train_loss : 0.44516   train_acc : 0.86179   valid_loss : 0.50459   valid_acc : 0.84375\n",
            "epoch 23 -- train_loss : 0.43903   train_acc : 0.86203   valid_loss : 0.48336   valid_acc : 0.85905\n",
            "epoch 24 -- train_loss : 0.41153   train_acc : 0.86686   valid_loss : 0.47375   valid_acc : 0.85547\n",
            "epoch 25 -- train_loss : 0.39299   train_acc : 0.87406   valid_loss : 0.46361   valid_acc : 0.85775\n",
            "epoch 26 -- train_loss : 0.37602   train_acc : 0.88313   valid_loss : 0.44558   valid_acc : 0.87598\n",
            "epoch 27 -- train_loss : 0.35829   train_acc : 0.88606   valid_loss : 0.43058   valid_acc : 0.88249\n",
            "epoch 28 -- train_loss : 0.35874   train_acc : 0.88618   valid_loss : 0.41198   valid_acc : 0.88770\n",
            "epoch 29 -- train_loss : 0.33401   train_acc : 0.89422   valid_loss : 0.40868   valid_acc : 0.86654\n",
            "epoch 30 -- train_loss : 0.31994   train_acc : 0.89998   valid_loss : 0.39997   valid_acc : 0.87923\n",
            "epoch 31 -- train_loss : 0.30475   train_acc : 0.90363   valid_loss : 0.38777   valid_acc : 0.88737\n",
            "epoch 32 -- train_loss : 0.29955   train_acc : 0.90471   valid_loss : 0.38198   valid_acc : 0.87402\n",
            "epoch 33 -- train_loss : 0.29679   train_acc : 0.90036   valid_loss : 0.36728   valid_acc : 0.88900\n",
            "epoch 34 -- train_loss : 0.27827   train_acc : 0.91179   valid_loss : 0.37935   valid_acc : 0.87109\n",
            "epoch 35 -- train_loss : 0.28078   train_acc : 0.90943   valid_loss : 0.36188   valid_acc : 0.86979\n",
            "epoch 36 -- train_loss : 0.26294   train_acc : 0.91632   valid_loss : 0.35527   valid_acc : 0.87793\n",
            "epoch 37 -- train_loss : 0.25568   train_acc : 0.91733   valid_loss : 0.33643   valid_acc : 0.90592\n",
            "epoch 38 -- train_loss : 0.24556   train_acc : 0.92494   valid_loss : 0.35997   valid_acc : 0.87923\n",
            "epoch 39 -- train_loss : 0.24723   train_acc : 0.91979   valid_loss : 0.35721   valid_acc : 0.88509\n",
            "epoch 40 -- train_loss : 0.23894   train_acc : 0.92181   valid_loss : 0.31067   valid_acc : 0.90495\n",
            "epoch 41 -- train_loss : 0.21835   train_acc : 0.93252   valid_loss : 0.32540   valid_acc : 0.88900\n",
            "epoch 42 -- train_loss : 0.22263   train_acc : 0.92883   valid_loss : 0.30518   valid_acc : 0.89909\n",
            "epoch 43 -- train_loss : 0.21370   train_acc : 0.92935   valid_loss : 0.30440   valid_acc : 0.89909\n",
            "epoch 44 -- train_loss : 0.20906   train_acc : 0.93038   valid_loss : 0.30428   valid_acc : 0.90234\n",
            "epoch 45 -- train_loss : 0.20339   train_acc : 0.93088   valid_loss : 0.29751   valid_acc : 0.90169\n",
            "epoch 46 -- train_loss : 0.19623   train_acc : 0.93396   valid_loss : 0.29892   valid_acc : 0.90072\n",
            "epoch 47 -- train_loss : 0.19279   train_acc : 0.93694   valid_loss : 0.29473   valid_acc : 0.90299\n",
            "epoch 48 -- train_loss : 0.18534   train_acc : 0.93865   valid_loss : 0.27974   valid_acc : 0.91309\n",
            "epoch 49 -- train_loss : 0.18007   train_acc : 0.94047   valid_loss : 0.28471   valid_acc : 0.90788\n",
            "epoch 50 -- train_loss : 0.17847   train_acc : 0.93865   valid_loss : 0.26512   valid_acc : 0.91471\n",
            "epoch 51 -- train_loss : 0.17404   train_acc : 0.94140   valid_loss : 0.27767   valid_acc : 0.91178\n",
            "epoch 52 -- train_loss : 0.16905   train_acc : 0.94193   valid_loss : 0.28298   valid_acc : 0.90853\n",
            "epoch 53 -- train_loss : 0.16926   train_acc : 0.94384   valid_loss : 0.27700   valid_acc : 0.90885\n",
            "epoch 54 -- train_loss : 0.15852   train_acc : 0.94620   valid_loss : 0.28906   valid_acc : 0.90560\n",
            "epoch 55 -- train_loss : 0.15486   train_acc : 0.94643   valid_loss : 0.25966   valid_acc : 0.90885\n",
            "epoch 56 -- train_loss : 0.15204   train_acc : 0.95039   valid_loss : 0.27283   valid_acc : 0.91178\n",
            "epoch 57 -- train_loss : 0.16312   train_acc : 0.94192   valid_loss : 0.25126   valid_acc : 0.91374\n",
            "epoch 58 -- train_loss : 0.14543   train_acc : 0.94951   valid_loss : 0.25333   valid_acc : 0.91699\n",
            "epoch 59 -- train_loss : 0.14195   train_acc : 0.95321   valid_loss : 0.24870   valid_acc : 0.91960\n",
            "epoch 60 -- train_loss : 0.14884   train_acc : 0.94670   valid_loss : 0.25275   valid_acc : 0.91536\n",
            "epoch 61 -- train_loss : 0.13440   train_acc : 0.95515   valid_loss : 0.26134   valid_acc : 0.91569\n",
            "epoch 62 -- train_loss : 0.13387   train_acc : 0.95641   valid_loss : 0.24253   valid_acc : 0.92057\n",
            "epoch 63 -- train_loss : 0.12756   train_acc : 0.95786   valid_loss : 0.25160   valid_acc : 0.91960\n",
            "epoch 64 -- train_loss : 0.12959   train_acc : 0.95530   valid_loss : 0.24423   valid_acc : 0.92155\n",
            "epoch 65 -- train_loss : 0.12981   train_acc : 0.95473   valid_loss : 0.25933   valid_acc : 0.91732\n",
            "epoch 66 -- train_loss : 0.12613   train_acc : 0.95722   valid_loss : 0.23582   valid_acc : 0.92350\n",
            "epoch 67 -- train_loss : 0.11923   train_acc : 0.96045   valid_loss : 0.22328   valid_acc : 0.92546\n",
            "epoch 68 -- train_loss : 0.12129   train_acc : 0.95851   valid_loss : 0.23023   valid_acc : 0.92546\n",
            "epoch 69 -- train_loss : 0.11747   train_acc : 0.95885   valid_loss : 0.23588   valid_acc : 0.91862\n",
            "epoch 70 -- train_loss : 0.11677   train_acc : 0.95945   valid_loss : 0.24025   valid_acc : 0.92220\n",
            "epoch 71 -- train_loss : 0.11260   train_acc : 0.96167   valid_loss : 0.25176   valid_acc : 0.90885\n",
            "epoch 72 -- train_loss : 0.10811   train_acc : 0.95961   valid_loss : 0.23485   valid_acc : 0.93424\n",
            "epoch 73 -- train_loss : 0.10662   train_acc : 0.96586   valid_loss : 0.24766   valid_acc : 0.90755\n",
            "epoch 74 -- train_loss : 0.10547   train_acc : 0.96498   valid_loss : 0.22636   valid_acc : 0.92839\n",
            "epoch 75 -- train_loss : 0.11240   train_acc : 0.95683   valid_loss : 0.25799   valid_acc : 0.90365\n",
            "epoch 76 -- train_loss : 0.11842   train_acc : 0.96033   valid_loss : 0.26721   valid_acc : 0.91471\n",
            "epoch 77 -- train_loss : 0.10292   train_acc : 0.96372   valid_loss : 0.23498   valid_acc : 0.91634\n",
            "epoch 78 -- train_loss : 0.10189   train_acc : 0.96529   valid_loss : 0.25635   valid_acc : 0.91667\n",
            "epoch 79 -- train_loss : 0.09862   train_acc : 0.96879   valid_loss : 0.22561   valid_acc : 0.92546\n",
            "epoch 80 -- train_loss : 0.09113   train_acc : 0.97100   valid_loss : 0.22989   valid_acc : 0.92839\n",
            "epoch 81 -- train_loss : 0.09911   train_acc : 0.96304   valid_loss : 0.24590   valid_acc : 0.91829\n",
            "epoch 82 -- train_loss : 0.09657   train_acc : 0.96505   valid_loss : 0.23716   valid_acc : 0.92253\n",
            "epoch 83 -- train_loss : 0.10294   train_acc : 0.96549   valid_loss : 0.24114   valid_acc : 0.92350\n",
            "epoch 84 -- train_loss : 0.09969   train_acc : 0.96426   valid_loss : 0.23900   valid_acc : 0.91341\n",
            "epoch 85 -- train_loss : 0.09180   train_acc : 0.96773   valid_loss : 0.22871   valid_acc : 0.93229\n",
            "epoch 86 -- train_loss : 0.08597   train_acc : 0.96834   valid_loss : 0.23392   valid_acc : 0.92741\n",
            "epoch 87 -- train_loss : 0.08174   train_acc : 0.97283   valid_loss : 0.22964   valid_acc : 0.92350\n",
            "epoch 88 -- train_loss : 0.08200   train_acc : 0.97268   valid_loss : 0.21747   valid_acc : 0.92643\n",
            "epoch 89 -- train_loss : 0.08920   train_acc : 0.96868   valid_loss : 0.22798   valid_acc : 0.92220\n",
            "epoch 90 -- train_loss : 0.08633   train_acc : 0.97150   valid_loss : 0.23119   valid_acc : 0.93229\n",
            "epoch 91 -- train_loss : 0.08535   train_acc : 0.97077   valid_loss : 0.22801   valid_acc : 0.92057\n",
            "epoch 92 -- train_loss : 0.08101   train_acc : 0.97169   valid_loss : 0.22782   valid_acc : 0.92741\n",
            "epoch 93 -- train_loss : 0.07905   train_acc : 0.97260   valid_loss : 0.22359   valid_acc : 0.93424\n",
            "epoch 94 -- train_loss : 0.08079   train_acc : 0.97336   valid_loss : 0.23946   valid_acc : 0.91764\n",
            "epoch 95 -- train_loss : 0.07897   train_acc : 0.97089   valid_loss : 0.24663   valid_acc : 0.92350\n",
            "epoch 96 -- train_loss : 0.08123   train_acc : 0.97071   valid_loss : 0.22477   valid_acc : 0.93262\n",
            "epoch 97 -- train_loss : 0.08106   train_acc : 0.96883   valid_loss : 0.22332   valid_acc : 0.92741\n",
            "epoch 98 -- train_loss : 0.08176   train_acc : 0.97089   valid_loss : 0.20585   valid_acc : 0.94141\n",
            "epoch 99 -- train_loss : 0.07516   train_acc : 0.97398   valid_loss : 0.23444   valid_acc : 0.93229\n",
            "epoch 100 -- train_loss : 0.07982   train_acc : 0.97134   valid_loss : 0.23585   valid_acc : 0.92839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CNN_network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN_network,self).__init__()\n",
        "        self.kernel = [2,3,4]\n",
        "        self.output_size = 128\n",
        "\n",
        "        #Convolution layer\n",
        "        self.conv1 = nn.Conv1d(in_channels = 2, out_channels =self.output_size, kernel_size = self.kernel[0], stride=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels = 2, out_channels =self.output_size, kernel_size = self.kernel[1], stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels = 2, out_channels =self.output_size, kernel_size = self.kernel[2], stride=1)\n",
        "\n",
        "        #pooling layer\n",
        "        self.pool1 = nn.MaxPool1d(self.kernel[0],stride = 1)\n",
        "        self.pool2 = nn.MaxPool1d(self.kernel[1],stride = 1)\n",
        "        self.pool3 = nn.MaxPool1d(self.kernel[2],stride = 1)\n",
        "\n",
        "        #Dropout & FC layer\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.linear1 = nn.Linear(8064,1024)\n",
        "        self.linear2 = nn.Linear(1024,128)\n",
        "        self.linear3 = nn.Linear(128,47)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x1 = self.conv1(x)\n",
        "        x1 = self.pool1(x1)\n",
        "\n",
        "\n",
        "        x2 = self.conv2(x)\n",
        "        x2 = self.pool2(x2)\n",
        "\n",
        "        x3 = self.conv3(x)\n",
        "        x3 = self.pool3(x3)\n",
        "\n",
        "        x_concat = torch.cat((x1,x2,x3),2)\n",
        "        x_concat = torch.flatten(x_concat,1)\n",
        "\n",
        "        out = self.linear1(x_concat)\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "liRYDNPw9Xqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_network()\n",
        "print(model(next(iter(TrainDataloader))[0]).shape)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKZGLoyY9m9W",
        "outputId": "45744452-bb03-4c21-862c-b902d9357ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 47])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_network(\n",
              "  (conv1): Conv1d(2, 128, kernel_size=(2,), stride=(1,))\n",
              "  (conv2): Conv1d(2, 128, kernel_size=(3,), stride=(1,))\n",
              "  (conv3): Conv1d(2, 128, kernel_size=(4,), stride=(1,))\n",
              "  (pool1): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (pool2): MaxPool1d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (pool3): MaxPool1d(kernel_size=4, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (linear1): Linear(in_features=8064, out_features=1024, bias=True)\n",
              "  (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
              "  (linear3): Linear(in_features=128, out_features=47, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 455
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  valid_loss = []\n",
        "  valid_acc = []\n",
        "\n",
        "  for skeleton, cls in iter(TrainDataloader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "\n",
        "    output = model(skeleton)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss.append(loss.item())\n",
        "    train_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for skeleton, cls in iter(ValidDataloader):\n",
        "      skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "      output = model(skeleton)\n",
        "      loss = criterion(output, cls)\n",
        "      valid_loss.append(loss.item())\n",
        "      valid_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "\n",
        "  print(f'epoch {epoch+1} -- train_loss : {sum(train_loss[-len(TrainDataloader):]) / len(TrainDataloader):.5f} \\\n",
        "  train_acc : {sum(train_acc[-len(TrainDataloader):]) / len(TrainDataloader):.5f}\\\n",
        "   valid_loss : {sum(valid_loss[-len(ValidDataloader):]) / len(ValidDataloader):.5f} \\\n",
        "  valid_acc : {sum(valid_acc[-len(ValidDataloader):]) / len(ValidDataloader):.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za0jZoAMvnTm",
        "outputId": "bd86dd2c-0d1b-4bcd-f8d8-7ae92e68a00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 -- train_loss : 5.94230   train_acc : 0.13236   valid_loss : 2.86244   valid_acc : 0.26823\n",
            "epoch 2 -- train_loss : 2.47795   train_acc : 0.32803   valid_loss : 2.19817   valid_acc : 0.37370\n",
            "epoch 3 -- train_loss : 1.89937   train_acc : 0.44308   valid_loss : 1.59647   valid_acc : 0.50358\n",
            "epoch 4 -- train_loss : 1.52847   train_acc : 0.52450   valid_loss : 1.38278   valid_acc : 0.55143\n",
            "epoch 5 -- train_loss : 1.23272   train_acc : 0.60464   valid_loss : 1.16501   valid_acc : 0.59538\n",
            "epoch 6 -- train_loss : 1.01281   train_acc : 0.66453   valid_loss : 1.03604   valid_acc : 0.65234\n",
            "epoch 7 -- train_loss : 0.90995   train_acc : 0.69765   valid_loss : 0.83355   valid_acc : 0.73665\n",
            "epoch 8 -- train_loss : 0.82497   train_acc : 0.72234   valid_loss : 0.80071   valid_acc : 0.75684\n",
            "epoch 9 -- train_loss : 0.76278   train_acc : 0.74341   valid_loss : 0.86582   valid_acc : 0.73079\n",
            "epoch 10 -- train_loss : 0.70594   train_acc : 0.75461   valid_loss : 0.79965   valid_acc : 0.73242\n",
            "epoch 11 -- train_loss : 0.60896   train_acc : 0.78685   valid_loss : 0.61825   valid_acc : 0.80208\n",
            "epoch 12 -- train_loss : 0.61641   train_acc : 0.78293   valid_loss : 0.73101   valid_acc : 0.75521\n",
            "epoch 13 -- train_loss : 0.61096   train_acc : 0.78867   valid_loss : 0.63117   valid_acc : 0.78841\n",
            "epoch 14 -- train_loss : 0.50865   train_acc : 0.82422   valid_loss : 0.59363   valid_acc : 0.80697\n",
            "epoch 15 -- train_loss : 0.49683   train_acc : 0.82659   valid_loss : 0.54178   valid_acc : 0.81152\n",
            "epoch 16 -- train_loss : 0.45177   train_acc : 0.83131   valid_loss : 0.62379   valid_acc : 0.79427\n",
            "epoch 17 -- train_loss : 0.43022   train_acc : 0.84674   valid_loss : 0.45633   valid_acc : 0.84831\n",
            "epoch 18 -- train_loss : 0.36953   train_acc : 0.86721   valid_loss : 0.51689   valid_acc : 0.81445\n",
            "epoch 19 -- train_loss : 0.39164   train_acc : 0.85859   valid_loss : 0.43322   valid_acc : 0.85840\n",
            "epoch 20 -- train_loss : 0.33565   train_acc : 0.86870   valid_loss : 0.43069   valid_acc : 0.84961\n",
            "epoch 21 -- train_loss : 0.36915   train_acc : 0.87133   valid_loss : 0.45595   valid_acc : 0.85482\n",
            "epoch 22 -- train_loss : 0.35169   train_acc : 0.87358   valid_loss : 0.36987   valid_acc : 0.88086\n",
            "epoch 23 -- train_loss : 0.32411   train_acc : 0.88264   valid_loss : 0.44004   valid_acc : 0.86426\n",
            "epoch 24 -- train_loss : 0.31003   train_acc : 0.88557   valid_loss : 0.51232   valid_acc : 0.83854\n",
            "epoch 25 -- train_loss : 0.34073   train_acc : 0.87155   valid_loss : 0.49726   valid_acc : 0.84408\n",
            "epoch 26 -- train_loss : 0.31596   train_acc : 0.88145   valid_loss : 0.43380   valid_acc : 0.83984\n",
            "epoch 27 -- train_loss : 0.31446   train_acc : 0.88404   valid_loss : 0.40161   valid_acc : 0.85645\n",
            "epoch 28 -- train_loss : 0.28364   train_acc : 0.90112   valid_loss : 0.32653   valid_acc : 0.90495\n",
            "epoch 29 -- train_loss : 0.25448   train_acc : 0.90398   valid_loss : 0.43917   valid_acc : 0.86100\n",
            "epoch 30 -- train_loss : 0.26202   train_acc : 0.90523   valid_loss : 0.35444   valid_acc : 0.89583\n",
            "epoch 31 -- train_loss : 0.24443   train_acc : 0.91317   valid_loss : 0.43739   valid_acc : 0.85775\n",
            "epoch 32 -- train_loss : 0.29931   train_acc : 0.89236   valid_loss : 0.41741   valid_acc : 0.87044\n",
            "epoch 33 -- train_loss : 0.23567   train_acc : 0.91294   valid_loss : 0.34455   valid_acc : 0.89518\n",
            "epoch 34 -- train_loss : 0.25652   train_acc : 0.90695   valid_loss : 0.37575   valid_acc : 0.88607\n",
            "epoch 35 -- train_loss : 0.24184   train_acc : 0.91087   valid_loss : 0.39216   valid_acc : 0.87337\n",
            "epoch 36 -- train_loss : 0.21885   train_acc : 0.91457   valid_loss : 0.28511   valid_acc : 0.91667\n",
            "epoch 37 -- train_loss : 0.20423   train_acc : 0.91827   valid_loss : 0.28691   valid_acc : 0.92090\n",
            "epoch 38 -- train_loss : 0.19191   train_acc : 0.92673   valid_loss : 0.36133   valid_acc : 0.89355\n",
            "epoch 39 -- train_loss : 0.19648   train_acc : 0.92448   valid_loss : 0.33692   valid_acc : 0.90104\n",
            "epoch 40 -- train_loss : 0.23175   train_acc : 0.91701   valid_loss : 0.43464   valid_acc : 0.87500\n",
            "epoch 41 -- train_loss : 0.24135   train_acc : 0.91358   valid_loss : 0.45725   valid_acc : 0.87793\n",
            "epoch 42 -- train_loss : 0.22701   train_acc : 0.91339   valid_loss : 0.33972   valid_acc : 0.90365\n",
            "epoch 43 -- train_loss : 0.24357   train_acc : 0.91270   valid_loss : 0.35749   valid_acc : 0.89486\n",
            "epoch 44 -- train_loss : 0.22937   train_acc : 0.91728   valid_loss : 0.39049   valid_acc : 0.89225\n",
            "epoch 45 -- train_loss : 0.24299   train_acc : 0.91495   valid_loss : 0.37936   valid_acc : 0.88770\n",
            "epoch 46 -- train_loss : 0.22572   train_acc : 0.91857   valid_loss : 0.34899   valid_acc : 0.90039\n",
            "epoch 47 -- train_loss : 0.20174   train_acc : 0.92093   valid_loss : 0.33512   valid_acc : 0.89746\n",
            "epoch 48 -- train_loss : 0.24231   train_acc : 0.91381   valid_loss : 0.30505   valid_acc : 0.90723\n",
            "epoch 49 -- train_loss : 0.23617   train_acc : 0.91545   valid_loss : 0.38017   valid_acc : 0.89095\n",
            "epoch 50 -- train_loss : 0.20907   train_acc : 0.91929   valid_loss : 0.37966   valid_acc : 0.89160\n",
            "epoch 51 -- train_loss : 0.17372   train_acc : 0.93312   valid_loss : 0.27758   valid_acc : 0.92480\n",
            "epoch 52 -- train_loss : 0.16042   train_acc : 0.93713   valid_loss : 0.27686   valid_acc : 0.91439\n",
            "epoch 53 -- train_loss : 0.18252   train_acc : 0.92989   valid_loss : 0.33012   valid_acc : 0.90365\n",
            "epoch 54 -- train_loss : 0.17842   train_acc : 0.93237   valid_loss : 0.31679   valid_acc : 0.91309\n",
            "epoch 55 -- train_loss : 0.21778   train_acc : 0.91656   valid_loss : 0.38903   valid_acc : 0.90072\n",
            "epoch 56 -- train_loss : 0.18941   train_acc : 0.93492   valid_loss : 0.24504   valid_acc : 0.93652\n",
            "epoch 57 -- train_loss : 0.16092   train_acc : 0.93706   valid_loss : 0.34591   valid_acc : 0.90690\n",
            "epoch 58 -- train_loss : 0.17776   train_acc : 0.93317   valid_loss : 0.33937   valid_acc : 0.91048\n",
            "epoch 59 -- train_loss : 0.17194   train_acc : 0.93587   valid_loss : 0.31236   valid_acc : 0.91276\n",
            "epoch 60 -- train_loss : 0.18653   train_acc : 0.92974   valid_loss : 0.23586   valid_acc : 0.93815\n",
            "epoch 61 -- train_loss : 0.16729   train_acc : 0.93751   valid_loss : 0.29758   valid_acc : 0.91439\n",
            "epoch 62 -- train_loss : 0.19865   train_acc : 0.92741   valid_loss : 0.38424   valid_acc : 0.88184\n",
            "epoch 63 -- train_loss : 0.22086   train_acc : 0.92440   valid_loss : 0.40698   valid_acc : 0.89258\n",
            "epoch 64 -- train_loss : 0.20468   train_acc : 0.92627   valid_loss : 0.37728   valid_acc : 0.90560\n",
            "epoch 65 -- train_loss : 0.19380   train_acc : 0.93191   valid_loss : 0.31161   valid_acc : 0.91602\n",
            "epoch 66 -- train_loss : 0.15127   train_acc : 0.94228   valid_loss : 0.28897   valid_acc : 0.91341\n",
            "epoch 67 -- train_loss : 0.13155   train_acc : 0.94863   valid_loss : 0.23396   valid_acc : 0.93424\n",
            "epoch 68 -- train_loss : 0.15010   train_acc : 0.94254   valid_loss : 0.31574   valid_acc : 0.91732\n",
            "epoch 69 -- train_loss : 0.16850   train_acc : 0.93534   valid_loss : 0.36062   valid_acc : 0.91667\n",
            "epoch 70 -- train_loss : 0.18280   train_acc : 0.93305   valid_loss : 0.28158   valid_acc : 0.92806\n",
            "epoch 71 -- train_loss : 0.15736   train_acc : 0.94338   valid_loss : 0.38898   valid_acc : 0.91081\n",
            "epoch 72 -- train_loss : 0.21597   train_acc : 0.92905   valid_loss : 0.39883   valid_acc : 0.89909\n",
            "epoch 73 -- train_loss : 0.19768   train_acc : 0.92801   valid_loss : 0.37107   valid_acc : 0.90690\n",
            "epoch 74 -- train_loss : 0.23183   train_acc : 0.92238   valid_loss : 0.37802   valid_acc : 0.89681\n",
            "epoch 75 -- train_loss : 0.16787   train_acc : 0.93667   valid_loss : 0.33784   valid_acc : 0.91276\n",
            "epoch 76 -- train_loss : 0.18146   train_acc : 0.93400   valid_loss : 0.33506   valid_acc : 0.90820\n",
            "epoch 77 -- train_loss : 0.17043   train_acc : 0.93834   valid_loss : 0.30599   valid_acc : 0.92448\n",
            "epoch 78 -- train_loss : 0.14251   train_acc : 0.94730   valid_loss : 0.26202   valid_acc : 0.92936\n",
            "epoch 79 -- train_loss : 0.14183   train_acc : 0.94346   valid_loss : 0.30728   valid_acc : 0.92057\n",
            "epoch 80 -- train_loss : 0.20672   train_acc : 0.93549   valid_loss : 0.50100   valid_acc : 0.85840\n",
            "epoch 81 -- train_loss : 0.21060   train_acc : 0.92768   valid_loss : 0.37757   valid_acc : 0.90332\n",
            "epoch 82 -- train_loss : 0.19149   train_acc : 0.93302   valid_loss : 0.38723   valid_acc : 0.90299\n",
            "epoch 83 -- train_loss : 0.17458   train_acc : 0.93918   valid_loss : 0.30347   valid_acc : 0.92253\n",
            "epoch 84 -- train_loss : 0.15574   train_acc : 0.94124   valid_loss : 0.30697   valid_acc : 0.93229\n",
            "epoch 85 -- train_loss : 0.16171   train_acc : 0.93920   valid_loss : 0.35360   valid_acc : 0.91536\n",
            "epoch 86 -- train_loss : 0.16418   train_acc : 0.94014   valid_loss : 0.31473   valid_acc : 0.93262\n",
            "epoch 87 -- train_loss : 0.16920   train_acc : 0.94098   valid_loss : 0.34539   valid_acc : 0.91732\n",
            "epoch 88 -- train_loss : 0.14802   train_acc : 0.94643   valid_loss : 0.29154   valid_acc : 0.92220\n",
            "epoch 89 -- train_loss : 0.17269   train_acc : 0.94037   valid_loss : 0.34963   valid_acc : 0.91471\n",
            "epoch 90 -- train_loss : 0.15715   train_acc : 0.94364   valid_loss : 0.39708   valid_acc : 0.90853\n",
            "epoch 91 -- train_loss : 0.19317   train_acc : 0.93583   valid_loss : 0.25829   valid_acc : 0.93262\n",
            "epoch 92 -- train_loss : 0.15727   train_acc : 0.94337   valid_loss : 0.37958   valid_acc : 0.89453\n",
            "epoch 93 -- train_loss : 0.18375   train_acc : 0.93824   valid_loss : 0.34211   valid_acc : 0.90592\n",
            "epoch 94 -- train_loss : 0.18466   train_acc : 0.93701   valid_loss : 0.25917   valid_acc : 0.92741\n",
            "epoch 95 -- train_loss : 0.18727   train_acc : 0.93975   valid_loss : 0.27188   valid_acc : 0.91862\n",
            "epoch 96 -- train_loss : 0.13852   train_acc : 0.94897   valid_loss : 0.38160   valid_acc : 0.90462\n",
            "epoch 97 -- train_loss : 0.17483   train_acc : 0.93983   valid_loss : 0.28658   valid_acc : 0.91960\n",
            "epoch 98 -- train_loss : 0.16858   train_acc : 0.94170   valid_loss : 0.33224   valid_acc : 0.91536\n",
            "epoch 99 -- train_loss : 0.18321   train_acc : 0.94269   valid_loss : 0.38275   valid_acc : 0.90397\n",
            "epoch 100 -- train_loss : 0.19580   train_acc : 0.93572   valid_loss : 0.36481   valid_acc : 0.91732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN with conv2d\n",
        "- epoch 100, 0.95"
      ],
      "metadata": {
        "id": "P5HhwcnwCtTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN2d(nn.Module):\n",
        "  def __init__(self) :\n",
        "    super(CNN2d, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, padding = 1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.conv3 = nn.Conv2d(64, 64, 3)\n",
        "    self.conv4 = nn.Conv2d(64, 128, 3)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "    self.fc1 = nn.Linear(768, 128)\n",
        "    self.fc2 = nn.Linear(128,256)\n",
        "    self.fc3 = nn.Linear(256,47)\n",
        "    self.drop = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = x.flatten(start_dim = 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "dXi279vUCuay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN2d()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  valid_loss = []\n",
        "  valid_acc = []\n",
        "\n",
        "  for skeleton, cls in iter(TrainDataloader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "\n",
        "    output = model(skeleton.unsqueeze(1))\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss.append(loss.item())\n",
        "    train_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for skeleton, cls in iter(ValidDataloader):\n",
        "      skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "      output = model(skeleton.unsqueeze(1))\n",
        "      loss = criterion(output, cls)\n",
        "      valid_loss.append(loss.item())\n",
        "      valid_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "\n",
        "  print(f'epoch {epoch+1} -- train_loss : {sum(train_loss[-len(TrainDataloader):]) / len(TrainDataloader):.5f} \\\n",
        "  train_acc : {sum(train_acc[-len(TrainDataloader):]) / len(TrainDataloader):.5f}\\\n",
        "   valid_loss : {sum(valid_loss[-len(ValidDataloader):]) / len(ValidDataloader):.5f} \\\n",
        "  valid_acc : {sum(valid_acc[-len(ValidDataloader):]) / len(ValidDataloader):.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2w_vdRcC0s7",
        "outputId": "65af0121-7e20-4dfd-9a3b-8040a84d39cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 -- train_loss : 3.31274   train_acc : 0.19105   valid_loss : 2.83573   valid_acc : 0.26530\n",
            "epoch 2 -- train_loss : 2.37152   train_acc : 0.35911   valid_loss : 1.89673   valid_acc : 0.46322\n",
            "epoch 3 -- train_loss : 1.55866   train_acc : 0.54163   valid_loss : 1.36447   valid_acc : 0.56901\n",
            "epoch 4 -- train_loss : 1.07648   train_acc : 0.65700   valid_loss : 0.87632   valid_acc : 0.72266\n",
            "epoch 5 -- train_loss : 0.78096   train_acc : 0.75239   valid_loss : 0.73884   valid_acc : 0.76400\n",
            "epoch 6 -- train_loss : 0.67484   train_acc : 0.77791   valid_loss : 0.70109   valid_acc : 0.75423\n",
            "epoch 7 -- train_loss : 0.58014   train_acc : 0.80678   valid_loss : 0.50901   valid_acc : 0.84635\n",
            "epoch 8 -- train_loss : 0.47815   train_acc : 0.84282   valid_loss : 0.48043   valid_acc : 0.84342\n",
            "epoch 9 -- train_loss : 0.46711   train_acc : 0.83958   valid_loss : 0.45843   valid_acc : 0.84701\n",
            "epoch 10 -- train_loss : 0.42000   train_acc : 0.84805   valid_loss : 0.41661   valid_acc : 0.85547\n",
            "epoch 11 -- train_loss : 0.37738   train_acc : 0.86919   valid_loss : 0.40674   valid_acc : 0.86947\n",
            "epoch 12 -- train_loss : 0.34260   train_acc : 0.87803   valid_loss : 0.35657   valid_acc : 0.88607\n",
            "epoch 13 -- train_loss : 0.34381   train_acc : 0.87761   valid_loss : 0.31522   valid_acc : 0.88997\n",
            "epoch 14 -- train_loss : 0.30981   train_acc : 0.88874   valid_loss : 0.41676   valid_acc : 0.85124\n",
            "epoch 15 -- train_loss : 0.27874   train_acc : 0.89647   valid_loss : 0.30500   valid_acc : 0.90039\n",
            "epoch 16 -- train_loss : 0.27412   train_acc : 0.90280   valid_loss : 0.28565   valid_acc : 0.90007\n",
            "epoch 17 -- train_loss : 0.29649   train_acc : 0.89300   valid_loss : 0.29665   valid_acc : 0.90169\n",
            "epoch 18 -- train_loss : 0.25131   train_acc : 0.90714   valid_loss : 0.30031   valid_acc : 0.88672\n",
            "epoch 19 -- train_loss : 0.25792   train_acc : 0.90287   valid_loss : 0.28109   valid_acc : 0.90104\n",
            "epoch 20 -- train_loss : 0.21526   train_acc : 0.91637   valid_loss : 0.23719   valid_acc : 0.91439\n",
            "epoch 21 -- train_loss : 0.20695   train_acc : 0.92087   valid_loss : 0.25939   valid_acc : 0.90560\n",
            "epoch 22 -- train_loss : 0.25291   train_acc : 0.90148   valid_loss : 0.32075   valid_acc : 0.89681\n",
            "epoch 23 -- train_loss : 0.24115   train_acc : 0.90965   valid_loss : 0.28675   valid_acc : 0.90723\n",
            "epoch 24 -- train_loss : 0.21039   train_acc : 0.91820   valid_loss : 0.23065   valid_acc : 0.91634\n",
            "epoch 25 -- train_loss : 0.18778   train_acc : 0.92710   valid_loss : 0.18653   valid_acc : 0.93327\n",
            "epoch 26 -- train_loss : 0.18515   train_acc : 0.92848   valid_loss : 0.22279   valid_acc : 0.91764\n",
            "epoch 27 -- train_loss : 0.18118   train_acc : 0.93107   valid_loss : 0.21626   valid_acc : 0.91927\n",
            "epoch 28 -- train_loss : 0.17847   train_acc : 0.92955   valid_loss : 0.20311   valid_acc : 0.92253\n",
            "epoch 29 -- train_loss : 0.18613   train_acc : 0.92986   valid_loss : 0.25015   valid_acc : 0.90397\n",
            "epoch 30 -- train_loss : 0.18378   train_acc : 0.92966   valid_loss : 0.26545   valid_acc : 0.90267\n",
            "epoch 31 -- train_loss : 0.15704   train_acc : 0.93484   valid_loss : 0.26214   valid_acc : 0.90267\n",
            "epoch 32 -- train_loss : 0.17069   train_acc : 0.93308   valid_loss : 0.18314   valid_acc : 0.92936\n",
            "epoch 33 -- train_loss : 0.18589   train_acc : 0.92887   valid_loss : 0.22994   valid_acc : 0.92839\n",
            "epoch 34 -- train_loss : 0.18796   train_acc : 0.92448   valid_loss : 0.22516   valid_acc : 0.92318\n",
            "epoch 35 -- train_loss : 0.15950   train_acc : 0.93797   valid_loss : 0.19025   valid_acc : 0.93424\n",
            "epoch 36 -- train_loss : 0.15519   train_acc : 0.94353   valid_loss : 0.14464   valid_acc : 0.94922\n",
            "epoch 37 -- train_loss : 0.15596   train_acc : 0.93472   valid_loss : 0.18537   valid_acc : 0.93848\n",
            "epoch 38 -- train_loss : 0.14799   train_acc : 0.94056   valid_loss : 0.23826   valid_acc : 0.90560\n",
            "epoch 39 -- train_loss : 0.14374   train_acc : 0.94311   valid_loss : 0.17196   valid_acc : 0.93034\n",
            "epoch 40 -- train_loss : 0.13945   train_acc : 0.94311   valid_loss : 0.20233   valid_acc : 0.91732\n",
            "epoch 41 -- train_loss : 0.12817   train_acc : 0.94761   valid_loss : 0.14725   valid_acc : 0.94694\n",
            "epoch 42 -- train_loss : 0.16245   train_acc : 0.93663   valid_loss : 0.18655   valid_acc : 0.92708\n",
            "epoch 43 -- train_loss : 0.13017   train_acc : 0.94718   valid_loss : 0.18471   valid_acc : 0.92839\n",
            "epoch 44 -- train_loss : 0.14008   train_acc : 0.94456   valid_loss : 0.19060   valid_acc : 0.91960\n",
            "epoch 45 -- train_loss : 0.16091   train_acc : 0.93496   valid_loss : 0.18642   valid_acc : 0.93587\n",
            "epoch 46 -- train_loss : 0.15836   train_acc : 0.93584   valid_loss : 0.17886   valid_acc : 0.93652\n",
            "epoch 47 -- train_loss : 0.14371   train_acc : 0.94578   valid_loss : 0.19148   valid_acc : 0.93945\n",
            "epoch 48 -- train_loss : 0.12994   train_acc : 0.94559   valid_loss : 0.17062   valid_acc : 0.93001\n",
            "epoch 49 -- train_loss : 0.11937   train_acc : 0.95309   valid_loss : 0.16322   valid_acc : 0.94434\n",
            "epoch 50 -- train_loss : 0.12663   train_acc : 0.94680   valid_loss : 0.16610   valid_acc : 0.93099\n",
            "epoch 51 -- train_loss : 0.12458   train_acc : 0.95309   valid_loss : 0.17310   valid_acc : 0.93164\n",
            "epoch 52 -- train_loss : 0.10632   train_acc : 0.95809   valid_loss : 0.13812   valid_acc : 0.93783\n",
            "epoch 53 -- train_loss : 0.13428   train_acc : 0.94463   valid_loss : 0.15773   valid_acc : 0.94108\n",
            "epoch 54 -- train_loss : 0.15558   train_acc : 0.93888   valid_loss : 0.19755   valid_acc : 0.93099\n",
            "epoch 55 -- train_loss : 0.15135   train_acc : 0.94357   valid_loss : 0.14187   valid_acc : 0.95280\n",
            "epoch 56 -- train_loss : 0.12302   train_acc : 0.94921   valid_loss : 0.15515   valid_acc : 0.93587\n",
            "epoch 57 -- train_loss : 0.15266   train_acc : 0.94266   valid_loss : 0.21653   valid_acc : 0.92806\n",
            "epoch 58 -- train_loss : 0.11443   train_acc : 0.95630   valid_loss : 0.13647   valid_acc : 0.95801\n",
            "epoch 59 -- train_loss : 0.11466   train_acc : 0.95371   valid_loss : 0.15630   valid_acc : 0.94043\n",
            "epoch 60 -- train_loss : 0.11032   train_acc : 0.95398   valid_loss : 0.15145   valid_acc : 0.94466\n",
            "epoch 61 -- train_loss : 0.12814   train_acc : 0.94886   valid_loss : 0.24191   valid_acc : 0.91764\n",
            "epoch 62 -- train_loss : 0.13697   train_acc : 0.94582   valid_loss : 0.16796   valid_acc : 0.94629\n",
            "epoch 63 -- train_loss : 0.12930   train_acc : 0.94974   valid_loss : 0.16179   valid_acc : 0.93099\n",
            "epoch 64 -- train_loss : 0.12400   train_acc : 0.94764   valid_loss : 0.18159   valid_acc : 0.94336\n",
            "epoch 65 -- train_loss : 0.12815   train_acc : 0.94837   valid_loss : 0.19668   valid_acc : 0.92871\n",
            "epoch 66 -- train_loss : 0.12564   train_acc : 0.95202   valid_loss : 0.16742   valid_acc : 0.93294\n",
            "epoch 67 -- train_loss : 0.11646   train_acc : 0.95023   valid_loss : 0.14861   valid_acc : 0.94792\n",
            "epoch 68 -- train_loss : 0.09988   train_acc : 0.95850   valid_loss : 0.17026   valid_acc : 0.95703\n",
            "epoch 69 -- train_loss : 0.13104   train_acc : 0.94863   valid_loss : 0.16486   valid_acc : 0.94922\n",
            "epoch 70 -- train_loss : 0.12714   train_acc : 0.94997   valid_loss : 0.23850   valid_acc : 0.91927\n",
            "epoch 71 -- train_loss : 0.10694   train_acc : 0.95588   valid_loss : 0.14665   valid_acc : 0.94694\n",
            "epoch 72 -- train_loss : 0.12438   train_acc : 0.95100   valid_loss : 0.14012   valid_acc : 0.94596\n",
            "epoch 73 -- train_loss : 0.09566   train_acc : 0.96117   valid_loss : 0.13327   valid_acc : 0.94694\n",
            "epoch 74 -- train_loss : 0.08276   train_acc : 0.96391   valid_loss : 0.14979   valid_acc : 0.94922\n",
            "epoch 75 -- train_loss : 0.12361   train_acc : 0.95192   valid_loss : 0.16433   valid_acc : 0.94010\n",
            "epoch 76 -- train_loss : 0.15711   train_acc : 0.93990   valid_loss : 0.21579   valid_acc : 0.92936\n",
            "epoch 77 -- train_loss : 0.10936   train_acc : 0.95519   valid_loss : 0.11925   valid_acc : 0.95410\n",
            "epoch 78 -- train_loss : 0.11189   train_acc : 0.95885   valid_loss : 0.12061   valid_acc : 0.95443\n",
            "epoch 79 -- train_loss : 0.09943   train_acc : 0.95733   valid_loss : 0.15764   valid_acc : 0.94727\n",
            "epoch 80 -- train_loss : 0.10970   train_acc : 0.95668   valid_loss : 0.15258   valid_acc : 0.94727\n",
            "epoch 81 -- train_loss : 0.11425   train_acc : 0.95397   valid_loss : 0.13480   valid_acc : 0.95573\n",
            "epoch 82 -- train_loss : 0.09495   train_acc : 0.96194   valid_loss : 0.15428   valid_acc : 0.94629\n",
            "epoch 83 -- train_loss : 0.09549   train_acc : 0.95900   valid_loss : 0.13349   valid_acc : 0.95410\n",
            "epoch 84 -- train_loss : 0.10335   train_acc : 0.95683   valid_loss : 0.16404   valid_acc : 0.94206\n",
            "epoch 85 -- train_loss : 0.12327   train_acc : 0.94902   valid_loss : 0.16820   valid_acc : 0.94173\n",
            "epoch 86 -- train_loss : 0.11704   train_acc : 0.95252   valid_loss : 0.17996   valid_acc : 0.93848\n",
            "epoch 87 -- train_loss : 0.11617   train_acc : 0.95180   valid_loss : 0.16174   valid_acc : 0.94336\n",
            "epoch 88 -- train_loss : 0.10895   train_acc : 0.95527   valid_loss : 0.16774   valid_acc : 0.93880\n",
            "epoch 89 -- train_loss : 0.13901   train_acc : 0.94688   valid_loss : 0.14041   valid_acc : 0.94303\n",
            "epoch 90 -- train_loss : 0.10397   train_acc : 0.95835   valid_loss : 0.17713   valid_acc : 0.92904\n",
            "epoch 91 -- train_loss : 0.11435   train_acc : 0.95614   valid_loss : 0.15709   valid_acc : 0.94629\n",
            "epoch 92 -- train_loss : 0.09765   train_acc : 0.96250   valid_loss : 0.12913   valid_acc : 0.94792\n",
            "epoch 93 -- train_loss : 0.10130   train_acc : 0.96041   valid_loss : 0.13944   valid_acc : 0.94271\n",
            "epoch 94 -- train_loss : 0.09213   train_acc : 0.96369   valid_loss : 0.12226   valid_acc : 0.95378\n",
            "epoch 95 -- train_loss : 0.08563   train_acc : 0.96327   valid_loss : 0.18689   valid_acc : 0.92253\n",
            "epoch 96 -- train_loss : 0.14004   train_acc : 0.95287   valid_loss : 0.17932   valid_acc : 0.95410\n",
            "epoch 97 -- train_loss : 0.13522   train_acc : 0.94650   valid_loss : 0.20365   valid_acc : 0.92578\n",
            "epoch 98 -- train_loss : 0.09568   train_acc : 0.96064   valid_loss : 0.14095   valid_acc : 0.95280\n",
            "epoch 99 -- train_loss : 0.07960   train_acc : 0.96826   valid_loss : 0.13474   valid_acc : 0.94629\n",
            "epoch 100 -- train_loss : 0.07184   train_acc : 0.96811   valid_loss : 0.14978   valid_acc : 0.93457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained model\n",
        "- mobilenet_v2, epoch 100, 0.91"
      ],
      "metadata": {
        "id": "xgucQ8DxE3yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "model = torchvision.models.mobilenet_v2(pretrained = True)\n",
        "model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "model.classifier[1] = nn.Linear(in_features=1280, out_features=47, bias=True)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-FrumjtF1A8",
        "outputId": "c11c0dc2-7a8d-451b-ae77-12bf843f9126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): Conv2dNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=1280, out_features=47, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 500
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  valid_loss = []\n",
        "  valid_acc = []\n",
        "\n",
        "  for skeleton, cls in iter(TrainDataloader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "\n",
        "    output = model(skeleton.unsqueeze(1))\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss.append(loss.item())\n",
        "    train_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for skeleton, cls in iter(ValidDataloader):\n",
        "      skeleton, cls = skeleton.to(device), cls.to(device).long()\n",
        "      output = model(skeleton.unsqueeze(1))\n",
        "      loss = criterion(output, cls)\n",
        "      valid_loss.append(loss.item())\n",
        "      valid_acc.append(sum(torch.max(output, dim=1)[1] == cls) / skeleton.shape[0])\n",
        "\n",
        "  print(f'epoch {epoch+1} -- train_loss : {sum(train_loss[-len(TrainDataloader):]) / len(TrainDataloader):.5f} \\\n",
        "  train_acc : {sum(train_acc[-len(TrainDataloader):]) / len(TrainDataloader):.5f}\\\n",
        "   valid_loss : {sum(valid_loss[-len(ValidDataloader):]) / len(ValidDataloader):.5f} \\\n",
        "  valid_acc : {sum(valid_acc[-len(ValidDataloader):]) / len(ValidDataloader):.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKCy-MD-E3Gd",
        "outputId": "c60fc923-a494-41cc-b918-778f6dfa3afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 -- train_loss : 2.46935   train_acc : 0.34066   valid_loss : 1.60310   valid_acc : 0.49577\n",
            "epoch 2 -- train_loss : 1.25396   train_acc : 0.59874   valid_loss : 1.11170   valid_acc : 0.64062\n",
            "epoch 3 -- train_loss : 0.87850   train_acc : 0.69105   valid_loss : 0.91385   valid_acc : 0.68132\n",
            "epoch 4 -- train_loss : 0.70783   train_acc : 0.75171   valid_loss : 0.82565   valid_acc : 0.70215\n",
            "epoch 5 -- train_loss : 0.63448   train_acc : 0.76931   valid_loss : 0.73019   valid_acc : 0.76335\n",
            "epoch 6 -- train_loss : 0.58162   train_acc : 0.79789   valid_loss : 0.77123   valid_acc : 0.76400\n",
            "epoch 7 -- train_loss : 0.43288   train_acc : 0.84206   valid_loss : 0.65532   valid_acc : 0.77344\n",
            "epoch 8 -- train_loss : 0.41944   train_acc : 0.84923   valid_loss : 0.68675   valid_acc : 0.77572\n",
            "epoch 9 -- train_loss : 0.40475   train_acc : 0.85848   valid_loss : 0.65487   valid_acc : 0.79720\n",
            "epoch 10 -- train_loss : 0.37769   train_acc : 0.86732   valid_loss : 0.56914   valid_acc : 0.82487\n",
            "epoch 11 -- train_loss : 0.36723   train_acc : 0.86816   valid_loss : 0.57697   valid_acc : 0.81901\n",
            "epoch 12 -- train_loss : 0.33786   train_acc : 0.88188   valid_loss : 0.57489   valid_acc : 0.80697\n",
            "epoch 13 -- train_loss : 0.29599   train_acc : 0.89620   valid_loss : 0.58126   valid_acc : 0.82194\n",
            "epoch 14 -- train_loss : 0.25352   train_acc : 0.90778   valid_loss : 0.52746   valid_acc : 0.85319\n",
            "epoch 15 -- train_loss : 0.24841   train_acc : 0.90616   valid_loss : 0.41530   valid_acc : 0.85775\n",
            "epoch 16 -- train_loss : 0.19972   train_acc : 0.92814   valid_loss : 0.50947   valid_acc : 0.85905\n",
            "epoch 17 -- train_loss : 0.24404   train_acc : 0.91461   valid_loss : 0.46088   valid_acc : 0.83626\n",
            "epoch 18 -- train_loss : 0.20834   train_acc : 0.92284   valid_loss : 0.55879   valid_acc : 0.84277\n",
            "epoch 19 -- train_loss : 0.29084   train_acc : 0.89975   valid_loss : 0.47616   valid_acc : 0.85547\n",
            "epoch 20 -- train_loss : 0.23736   train_acc : 0.91408   valid_loss : 0.51775   valid_acc : 0.86230\n",
            "epoch 21 -- train_loss : 0.21129   train_acc : 0.92367   valid_loss : 0.54691   valid_acc : 0.83659\n",
            "epoch 22 -- train_loss : 0.21872   train_acc : 0.92300   valid_loss : 0.39399   valid_acc : 0.88249\n",
            "epoch 23 -- train_loss : 0.20885   train_acc : 0.92014   valid_loss : 0.45289   valid_acc : 0.86816\n",
            "epoch 24 -- train_loss : 0.21787   train_acc : 0.92424   valid_loss : 0.36864   valid_acc : 0.88704\n",
            "epoch 25 -- train_loss : 0.16863   train_acc : 0.93713   valid_loss : 0.38913   valid_acc : 0.88835\n",
            "epoch 26 -- train_loss : 0.17894   train_acc : 0.93569   valid_loss : 0.49157   valid_acc : 0.86035\n",
            "epoch 27 -- train_loss : 0.21418   train_acc : 0.92943   valid_loss : 0.45458   valid_acc : 0.88118\n",
            "epoch 28 -- train_loss : 0.19031   train_acc : 0.93076   valid_loss : 0.54741   valid_acc : 0.85384\n",
            "epoch 29 -- train_loss : 0.17812   train_acc : 0.93880   valid_loss : 0.43465   valid_acc : 0.87305\n",
            "epoch 30 -- train_loss : 0.13057   train_acc : 0.94989   valid_loss : 0.39419   valid_acc : 0.89258\n",
            "epoch 31 -- train_loss : 0.14553   train_acc : 0.94387   valid_loss : 0.42486   valid_acc : 0.88411\n",
            "epoch 32 -- train_loss : 0.16790   train_acc : 0.94174   valid_loss : 0.44109   valid_acc : 0.88509\n",
            "epoch 33 -- train_loss : 0.16140   train_acc : 0.94311   valid_loss : 0.41931   valid_acc : 0.89030\n",
            "epoch 34 -- train_loss : 0.13566   train_acc : 0.94955   valid_loss : 0.35842   valid_acc : 0.88770\n",
            "epoch 35 -- train_loss : 0.12602   train_acc : 0.95583   valid_loss : 0.45566   valid_acc : 0.87630\n",
            "epoch 36 -- train_loss : 0.17368   train_acc : 0.93815   valid_loss : 0.34324   valid_acc : 0.89876\n",
            "epoch 37 -- train_loss : 0.15189   train_acc : 0.94841   valid_loss : 0.41351   valid_acc : 0.88053\n",
            "epoch 38 -- train_loss : 0.16852   train_acc : 0.94030   valid_loss : 0.45530   valid_acc : 0.88281\n",
            "epoch 39 -- train_loss : 0.11632   train_acc : 0.95271   valid_loss : 0.43951   valid_acc : 0.87695\n",
            "epoch 40 -- train_loss : 0.10482   train_acc : 0.96109   valid_loss : 0.45924   valid_acc : 0.86816\n",
            "epoch 41 -- train_loss : 0.13565   train_acc : 0.95017   valid_loss : 0.39990   valid_acc : 0.87793\n",
            "epoch 42 -- train_loss : 0.14248   train_acc : 0.95081   valid_loss : 0.51950   valid_acc : 0.87240\n",
            "epoch 43 -- train_loss : 0.15145   train_acc : 0.94819   valid_loss : 0.39151   valid_acc : 0.88965\n",
            "epoch 44 -- train_loss : 0.14313   train_acc : 0.94662   valid_loss : 0.44411   valid_acc : 0.88932\n",
            "epoch 45 -- train_loss : 0.13330   train_acc : 0.95172   valid_loss : 0.45871   valid_acc : 0.88249\n",
            "epoch 46 -- train_loss : 0.14987   train_acc : 0.94567   valid_loss : 0.42443   valid_acc : 0.88216\n",
            "epoch 47 -- train_loss : 0.14420   train_acc : 0.94936   valid_loss : 0.50777   valid_acc : 0.86654\n",
            "epoch 48 -- train_loss : 0.12181   train_acc : 0.95424   valid_loss : 0.43888   valid_acc : 0.90527\n",
            "epoch 49 -- train_loss : 0.13651   train_acc : 0.95351   valid_loss : 0.49624   valid_acc : 0.87174\n",
            "epoch 50 -- train_loss : 0.16033   train_acc : 0.94608   valid_loss : 0.40785   valid_acc : 0.89030\n",
            "epoch 51 -- train_loss : 0.14760   train_acc : 0.94635   valid_loss : 0.31781   valid_acc : 0.91016\n",
            "epoch 52 -- train_loss : 0.10737   train_acc : 0.96136   valid_loss : 0.45708   valid_acc : 0.89453\n",
            "epoch 53 -- train_loss : 0.09742   train_acc : 0.96198   valid_loss : 0.35933   valid_acc : 0.91341\n",
            "epoch 54 -- train_loss : 0.12263   train_acc : 0.95507   valid_loss : 0.39067   valid_acc : 0.90039\n",
            "epoch 55 -- train_loss : 0.13398   train_acc : 0.95332   valid_loss : 0.48354   valid_acc : 0.89225\n",
            "epoch 56 -- train_loss : 0.16567   train_acc : 0.94738   valid_loss : 0.48952   valid_acc : 0.88118\n",
            "epoch 57 -- train_loss : 0.13205   train_acc : 0.95177   valid_loss : 0.44770   valid_acc : 0.87272\n",
            "epoch 58 -- train_loss : 0.11835   train_acc : 0.95885   valid_loss : 0.49245   valid_acc : 0.87923\n",
            "epoch 59 -- train_loss : 0.11870   train_acc : 0.95900   valid_loss : 0.46383   valid_acc : 0.87337\n",
            "epoch 60 -- train_loss : 0.09503   train_acc : 0.96117   valid_loss : 0.39963   valid_acc : 0.87435\n",
            "epoch 61 -- train_loss : 0.08747   train_acc : 0.96643   valid_loss : 0.32900   valid_acc : 0.90202\n",
            "epoch 62 -- train_loss : 0.07627   train_acc : 0.97177   valid_loss : 0.36906   valid_acc : 0.87891\n",
            "epoch 63 -- train_loss : 0.07723   train_acc : 0.97382   valid_loss : 0.42915   valid_acc : 0.90299\n",
            "epoch 64 -- train_loss : 0.10594   train_acc : 0.96403   valid_loss : 0.46202   valid_acc : 0.89583\n",
            "epoch 65 -- train_loss : 0.09299   train_acc : 0.96738   valid_loss : 0.28677   valid_acc : 0.92318\n",
            "epoch 66 -- train_loss : 0.10345   train_acc : 0.96575   valid_loss : 0.44024   valid_acc : 0.90267\n",
            "epoch 67 -- train_loss : 0.15364   train_acc : 0.94970   valid_loss : 0.49642   valid_acc : 0.86426\n",
            "epoch 68 -- train_loss : 0.14947   train_acc : 0.94940   valid_loss : 0.47820   valid_acc : 0.87988\n",
            "epoch 69 -- train_loss : 0.10676   train_acc : 0.96404   valid_loss : 0.41228   valid_acc : 0.90332\n",
            "epoch 70 -- train_loss : 0.15126   train_acc : 0.95107   valid_loss : 0.45546   valid_acc : 0.89258\n",
            "epoch 71 -- train_loss : 0.15285   train_acc : 0.94878   valid_loss : 0.37679   valid_acc : 0.89486\n",
            "epoch 72 -- train_loss : 0.11486   train_acc : 0.95893   valid_loss : 0.37358   valid_acc : 0.89779\n",
            "epoch 73 -- train_loss : 0.11373   train_acc : 0.95862   valid_loss : 0.48749   valid_acc : 0.88639\n",
            "epoch 74 -- train_loss : 0.12955   train_acc : 0.95854   valid_loss : 0.36853   valid_acc : 0.91146\n",
            "epoch 75 -- train_loss : 0.06968   train_acc : 0.97234   valid_loss : 0.33853   valid_acc : 0.91276\n",
            "epoch 76 -- train_loss : 0.07761   train_acc : 0.96868   valid_loss : 0.40048   valid_acc : 0.90885\n",
            "epoch 77 -- train_loss : 0.10507   train_acc : 0.96316   valid_loss : 0.39045   valid_acc : 0.90853\n",
            "epoch 78 -- train_loss : 0.09999   train_acc : 0.96529   valid_loss : 0.36682   valid_acc : 0.89811\n",
            "epoch 79 -- train_loss : 0.11687   train_acc : 0.96341   valid_loss : 0.34593   valid_acc : 0.90885\n",
            "epoch 80 -- train_loss : 0.08171   train_acc : 0.96818   valid_loss : 0.36382   valid_acc : 0.91211\n",
            "epoch 81 -- train_loss : 0.09943   train_acc : 0.96399   valid_loss : 0.48130   valid_acc : 0.89128\n",
            "epoch 82 -- train_loss : 0.14475   train_acc : 0.95549   valid_loss : 0.42197   valid_acc : 0.90592\n",
            "epoch 83 -- train_loss : 0.10921   train_acc : 0.96144   valid_loss : 0.38836   valid_acc : 0.89258\n",
            "epoch 84 -- train_loss : 0.08182   train_acc : 0.97112   valid_loss : 0.41600   valid_acc : 0.92090\n",
            "epoch 85 -- train_loss : 0.09982   train_acc : 0.96624   valid_loss : 0.30882   valid_acc : 0.90104\n",
            "epoch 86 -- train_loss : 0.08449   train_acc : 0.96483   valid_loss : 0.30947   valid_acc : 0.93913\n",
            "epoch 87 -- train_loss : 0.07674   train_acc : 0.97230   valid_loss : 0.31828   valid_acc : 0.90918\n",
            "epoch 88 -- train_loss : 0.07446   train_acc : 0.97458   valid_loss : 0.27357   valid_acc : 0.91797\n",
            "epoch 89 -- train_loss : 0.07540   train_acc : 0.97123   valid_loss : 0.34756   valid_acc : 0.91309\n",
            "epoch 90 -- train_loss : 0.06614   train_acc : 0.97515   valid_loss : 0.32591   valid_acc : 0.92448\n",
            "epoch 91 -- train_loss : 0.05683   train_acc : 0.97858   valid_loss : 0.28468   valid_acc : 0.91471\n",
            "epoch 92 -- train_loss : 0.05846   train_acc : 0.97942   valid_loss : 0.27187   valid_acc : 0.93294\n",
            "epoch 93 -- train_loss : 0.07260   train_acc : 0.98015   valid_loss : 0.31681   valid_acc : 0.92871\n",
            "epoch 94 -- train_loss : 0.07515   train_acc : 0.97432   valid_loss : 0.44117   valid_acc : 0.91927\n",
            "epoch 95 -- train_loss : 0.08087   train_acc : 0.97115   valid_loss : 0.36540   valid_acc : 0.90820\n",
            "epoch 96 -- train_loss : 0.10241   train_acc : 0.96780   valid_loss : 0.49272   valid_acc : 0.87695\n",
            "epoch 97 -- train_loss : 0.12255   train_acc : 0.95775   valid_loss : 0.55257   valid_acc : 0.90137\n",
            "epoch 98 -- train_loss : 0.15786   train_acc : 0.94849   valid_loss : 0.51607   valid_acc : 0.88184\n",
            "epoch 99 -- train_loss : 0.16363   train_acc : 0.94571   valid_loss : 0.42495   valid_acc : 0.89811\n",
            "epoch 100 -- train_loss : 0.10634   train_acc : 0.96731   valid_loss : 0.31834   valid_acc : 0.91081\n"
          ]
        }
      ]
    }
  ]
}